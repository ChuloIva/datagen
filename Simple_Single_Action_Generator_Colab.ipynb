{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Single Cognitive Action Data Generator\n",
        "\n",
        "Generate 7,000+ simple, first-person training examples for a single cognitive action.\n",
        "\n",
        "**Features:**\n",
        "- 🎯 Single cognitive action focus\n",
        "- 👤 First-person perspective only\n",
        "- 📝 Simple complexity examples\n",
        "- 💾 Auto-checkpointing every 250 examples\n",
        "- 🚀 Optimized for 16GB VRAM (uses gemma2:9b)\n",
        "- ⚡ Async parallel processing (8 concurrent requests)\n",
        "\n",
        "**Estimated Time:** ~2.5 hours for 7,000 examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1️⃣ Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q requests pandas numpy tqdm aiohttp nest-asyncio\n",
        "\n",
        "# Clone the repository\n",
        "import os\n",
        "if not os.path.exists('datagen'):\n",
        "    print(\"📥 Cloning datagen repository...\")\n",
        "    !git clone https://github.com/ChuloIva/datagen.git\n",
        "    print(\"✅ Repository cloned successfully!\")\n",
        "else:\n",
        "    print(\"✅ Repository already exists\")\n",
        "\n",
        "# Import libraries\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import nest_asyncio\n",
        "import requests\n",
        "import subprocess\n",
        "from typing import List, Dict, Any\n",
        "from dataclasses import dataclass, asdict\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Apply nest_asyncio for Jupyter/Colab compatibility\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Set random seeds\n",
        "random.seed(42)\n",
        "\n",
        "print(\"✅ Dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2️⃣ Install & Configure Ollama\n",
        "\n",
        "Using gemma2:9b model (~5GB VRAM) with 8 parallel requests (~12-14GB total)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "!curl -fsSL https://ollama.ai/install.sh | sh\n",
        "\n",
        "# Stop any existing Ollama processes\n",
        "print(\"🛑 Stopping any existing Ollama processes...\")\n",
        "subprocess.run(['pkill', '-9', 'ollama'], stderr=subprocess.DEVNULL)\n",
        "time.sleep(2)\n",
        "\n",
        "# Set environment variables for 16GB VRAM\n",
        "print(\"\\n⚙️  Configuring Ollama for 16GB VRAM...\")\n",
        "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "os.environ['OLLAMA_NUM_PARALLEL'] = '8'  # 8 parallel requests for 16GB\n",
        "os.environ['OLLAMA_MAX_QUEUE'] = '256'\n",
        "os.environ['OLLAMA_MAX_LOADED_MODELS'] = '1'\n",
        "os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia'\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model: gemma2:9b (~5GB)\")\n",
        "print(f\"  Parallel requests: 8\")\n",
        "print(f\"  Expected VRAM: 12-14GB\")\n",
        "\n",
        "# Start Ollama server\n",
        "print(\"\\n🚀 Starting Ollama server...\")\n",
        "subprocess.Popen(['ollama', 'serve'], \n",
        "                 env=os.environ.copy(),\n",
        "                 stdout=subprocess.DEVNULL,\n",
        "                 stderr=subprocess.DEVNULL)\n",
        "\n",
        "print(\"⏳ Waiting for Ollama to start...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# Verify Ollama is running\n",
        "try:\n",
        "    response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
        "    if response.status_code == 200:\n",
        "        print(\"✅ Ollama is running!\")\n",
        "    else:\n",
        "        print(\"❌ Ollama error\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Connection error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3️⃣ Pull the Model\n",
        "\n",
        "Download gemma2:9b model (~5.4GB download)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"📥 Pulling gemma2:9b model (this may take 3-5 minutes)...\")\n",
        "!ollama pull gemma2:9b\n",
        "print(\"\\n✅ Model ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4️⃣ Load Cognitive Actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add datagen to Python path\n",
        "import sys\n",
        "datagen_dir = os.path.abspath('datagen')\n",
        "if datagen_dir not in sys.path:\n",
        "    sys.path.insert(0, datagen_dir)\n",
        "\n",
        "# Import cognitive actions\n",
        "from variable_pools import COGNITIVE_ACTIONS\n",
        "\n",
        "print(f\"✅ Loaded {len(COGNITIVE_ACTIONS)} cognitive actions\\n\")\n",
        "print(\"Available cognitive actions:\")\n",
        "for idx, action in enumerate(COGNITIVE_ACTIONS.keys(), 1):\n",
        "    print(f\"{idx:2d}. {action}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5️⃣ Select Cognitive Action\n",
        "\n",
        "Choose which cognitive action to generate 7,000 examples for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SELECT YOUR COGNITIVE ACTION HERE\n",
        "SELECTED_ACTION = \"analyzing\"  # Change this to your desired action\n",
        "\n",
        "# Verify it exists\n",
        "if SELECTED_ACTION not in COGNITIVE_ACTIONS:\n",
        "    print(f\"❌ '{SELECTED_ACTION}' not found!\")\n",
        "    print(f\"Available actions: {list(COGNITIVE_ACTIONS.keys())}\")\n",
        "else:\n",
        "    action_desc = COGNITIVE_ACTIONS[SELECTED_ACTION]\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Selected: {SELECTED_ACTION}\")\n",
        "    print(f\"Description: {action_desc}\")\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6️⃣ Mount Google Drive (for checkpoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create checkpoint directory\n",
        "checkpoint_dir = f'/content/drive/MyDrive/cognitive_data_{SELECTED_ACTION}'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "print(f\"✅ Checkpoints will be saved to: {checkpoint_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7️⃣ Simple Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SimpleExample:\n",
        "    text: str\n",
        "    cognitive_action: str\n",
        "    domain: str\n",
        "    \n",
        "class SimpleDataGenerator:\n",
        "    def __init__(self, base_url=\"http://localhost:11434\", max_parallel=8):\n",
        "        self.base_url = base_url\n",
        "        self.max_parallel = max_parallel\n",
        "        self.examples = []\n",
        "        self.semaphore = asyncio.Semaphore(max_parallel)\n",
        "        \n",
        "        # Simple domains for variety\n",
        "        self.domains = [\n",
        "            \"work\", \"school\", \"daily life\", \"cooking\", \"shopping\",\n",
        "            \"exercise\", \"reading\", \"writing\", \"planning\", \"learning\",\n",
        "            \"organizing\", \"problem-solving\", \"hobbies\", \"personal goals\",\n",
        "            \"time management\", \"finances\", \"health\", \"relationships\",\n",
        "            \"home projects\", \"travel\"\n",
        "        ]\n",
        "    \n",
        "    def create_prompt(self, action: str, action_desc: str, domain: str) -> str:\n",
        "        \"\"\"Create simple first-person prompt.\"\"\"\n",
        "        return f\"\"\"Generate a simple, first-person example of someone {action}.\n",
        "\n",
        "Action: {action}\n",
        "Description: {action_desc}\n",
        "Domain: {domain}\n",
        "\n",
        "Requirements:\n",
        "- Write in first person (I, my, me)\n",
        "- Keep it simple and realistic\n",
        "- 2-4 sentences maximum\n",
        "- Focus on the {action} cognitive action\n",
        "- Use everyday language\n",
        "\n",
        "Example only (no explanation):\"\"\"\n",
        "    \n",
        "    async def generate_one(self, session: aiohttp.ClientSession, action: str, \n",
        "                          action_desc: str, domain: str, model: str) -> SimpleExample:\n",
        "        \"\"\"Generate one example.\"\"\"\n",
        "        async with self.semaphore:\n",
        "            prompt = self.create_prompt(action, action_desc, domain)\n",
        "            \n",
        "            try:\n",
        "                async with session.post(\n",
        "                    f\"{self.base_url}/api/generate\",\n",
        "                    json={\"model\": model, \"prompt\": prompt, \"stream\": False},\n",
        "                    timeout=aiohttp.ClientTimeout(total=60)\n",
        "                ) as response:\n",
        "                    result = await response.json()\n",
        "                    text = result.get('response', '').strip()\n",
        "                    \n",
        "                    # Clean up the text\n",
        "                    text = text.replace('\"', '').strip()\n",
        "                    if not text:\n",
        "                        return None\n",
        "                    \n",
        "                    return SimpleExample(\n",
        "                        text=text,\n",
        "                        cognitive_action=action,\n",
        "                        domain=domain\n",
        "                    )\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                return None\n",
        "    \n",
        "    async def generate_batch_async(self, count: int, action: str, \n",
        "                                  action_desc: str, model: str) -> List[SimpleExample]:\n",
        "        \"\"\"Generate a batch of examples.\"\"\"\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            tasks = []\n",
        "            for _ in range(count):\n",
        "                domain = random.choice(self.domains)\n",
        "                task = self.generate_one(session, action, action_desc, domain, model)\n",
        "                tasks.append(task)\n",
        "            \n",
        "            results = await asyncio.gather(*tasks)\n",
        "            return [r for r in results if r is not None]\n",
        "    \n",
        "    def generate_batch(self, count: int, action: str, action_desc: str, \n",
        "                      model: str) -> List[SimpleExample]:\n",
        "        \"\"\"Synchronous wrapper for batch generation.\"\"\"\n",
        "        examples = asyncio.run(self.generate_batch_async(count, action, action_desc, model))\n",
        "        self.examples.extend(examples)\n",
        "        return examples\n",
        "\n",
        "print(\"✅ Simple data generator ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8️⃣ Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    'total_examples': 7000,\n",
        "    'model': 'gemma2:9b',\n",
        "    'max_parallel': 8,\n",
        "    'checkpoint_interval': 250,\n",
        "    'checkpoint_dir': checkpoint_dir\n",
        "}\n",
        "\n",
        "estimated_hours = CONFIG['total_examples'] / CONFIG['max_parallel'] * 20 / 3600\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"GENERATION CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Cognitive action: {SELECTED_ACTION}\")\n",
        "print(f\"Total examples: {CONFIG['total_examples']:,}\")\n",
        "print(f\"Model: {CONFIG['model']} (~5GB VRAM)\")\n",
        "print(f\"Parallel requests: {CONFIG['max_parallel']}\")\n",
        "print(f\"Checkpoint every: {CONFIG['checkpoint_interval']} examples\")\n",
        "print(f\"\\nPerspective: First-person only\")\n",
        "print(f\"Complexity: Simple only\")\n",
        "print(f\"\\nEstimated time: {estimated_hours:.1f} hours ({estimated_hours*60:.0f} minutes)\")\n",
        "print(f\"Expected VRAM: 12-14GB\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9️⃣ Generate 7,000 Examples\n",
        "\n",
        "⚠️ **This will take ~2.5 hours. Checkpoints saved every 250 examples.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize generator\n",
        "generator = SimpleDataGenerator(max_parallel=CONFIG['max_parallel'])\n",
        "\n",
        "total_target = CONFIG['total_examples']\n",
        "checkpoint_interval = CONFIG['checkpoint_interval']\n",
        "action_desc = COGNITIVE_ACTIONS[SELECTED_ACTION]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"🚀 GENERATING {total_target:,} EXAMPLES FOR: {SELECTED_ACTION}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "checkpoint_counter = 0\n",
        "num_checkpoints = (total_target + checkpoint_interval - 1) // checkpoint_interval\n",
        "\n",
        "# Progress bar\n",
        "pbar = tqdm(total=total_target, desc=\"Generating\", unit=\"examples\")\n",
        "\n",
        "for checkpoint_idx in range(num_checkpoints):\n",
        "    batch_size = min(checkpoint_interval, total_target - len(generator.examples))\n",
        "    \n",
        "    # Generate batch\n",
        "    batch_examples = generator.generate_batch(\n",
        "        count=batch_size,\n",
        "        action=SELECTED_ACTION,\n",
        "        action_desc=action_desc,\n",
        "        model=CONFIG['model']\n",
        "    )\n",
        "    \n",
        "    # Update progress\n",
        "    pbar.update(len(batch_examples))\n",
        "    \n",
        "    # Save checkpoint\n",
        "    checkpoint_counter += 1\n",
        "    checkpoint_file = os.path.join(\n",
        "        CONFIG['checkpoint_dir'],\n",
        "        f\"checkpoint_{checkpoint_counter:04d}_{int(time.time())}.jsonl\"\n",
        "    )\n",
        "    \n",
        "    with open(checkpoint_file, 'w') as f:\n",
        "        for ex in batch_examples:\n",
        "            f.write(json.dumps(asdict(ex)) + '\\n')\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    rate = len(generator.examples) / elapsed if elapsed > 0 else 0\n",
        "    eta = (total_target - len(generator.examples)) / rate if rate > 0 else 0\n",
        "    \n",
        "    pbar.set_postfix({\n",
        "        'rate': f'{rate:.1f}/s',\n",
        "        'ETA': f'{eta/60:.0f}m'\n",
        "    })\n",
        "\n",
        "pbar.close()\n",
        "\n",
        "# Save final dataset\n",
        "elapsed = time.time() - start_time\n",
        "final_file = os.path.join(\n",
        "    CONFIG['checkpoint_dir'],\n",
        "    f\"{SELECTED_ACTION}_7k_final_{int(time.time())}.jsonl\"\n",
        ")\n",
        "\n",
        "with open(final_file, 'w') as f:\n",
        "    for ex in generator.examples:\n",
        "        f.write(json.dumps(asdict(ex)) + '\\n')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎉 GENERATION COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Cognitive action: {SELECTED_ACTION}\")\n",
        "print(f\"Examples generated: {len(generator.examples):,}\")\n",
        "print(f\"Time elapsed: {elapsed/3600:.2f} hours ({elapsed/60:.1f} minutes)\")\n",
        "print(f\"Average rate: {len(generator.examples)/elapsed:.1f} examples/sec\")\n",
        "print(f\"Checkpoints saved: {checkpoint_counter}\")\n",
        "print(f\"Final dataset: {final_file}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔟 Preview Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show random examples\n",
        "print(f\"\\n📝 Sample examples for '{SELECTED_ACTION}':\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sample_examples = random.sample(generator.examples, min(10, len(generator.examples)))\n",
        "for idx, ex in enumerate(sample_examples, 1):\n",
        "    print(f\"{idx}. [{ex.domain}]\")\n",
        "    print(f\"   {ex.text}\")\n",
        "    print()\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1️⃣1️⃣ Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Domain distribution\n",
        "domains = [ex.domain for ex in generator.examples]\n",
        "domain_counts = pd.Series(domains).value_counts()\n",
        "\n",
        "print(\"\\n📊 STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total examples: {len(generator.examples):,}\")\n",
        "print(f\"Unique domains: {len(domain_counts)}\")\n",
        "print(f\"\\nTop 10 domains:\")\n",
        "print(domain_counts.head(10))\n",
        "\n",
        "# Text length statistics\n",
        "text_lengths = [len(ex.text.split()) for ex in generator.examples]\n",
        "print(f\"\\nText length (words):\")\n",
        "print(f\"  Mean: {sum(text_lengths)/len(text_lengths):.1f}\")\n",
        "print(f\"  Min: {min(text_lengths)}\")\n",
        "print(f\"  Max: {max(text_lengths)}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1️⃣2️⃣ Download Final Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Find the final file\n",
        "import glob\n",
        "final_files = glob.glob(os.path.join(CONFIG['checkpoint_dir'], f\"{SELECTED_ACTION}_7k_final_*.jsonl\"))\n",
        "if final_files:\n",
        "    latest_file = max(final_files, key=os.path.getctime)\n",
        "    print(f\"Downloading: {os.path.basename(latest_file)}\")\n",
        "    files.download(latest_file)\n",
        "    print(\"✅ Download started!\")\n",
        "else:\n",
        "    print(\"❌ No final file found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎉 Done!\n",
        "\n",
        "You now have 7,000+ simple, first-person examples for **{SELECTED_ACTION}**!\n",
        "\n",
        "### Dataset saved to:\n",
        "- **Google Drive**: `/content/drive/MyDrive/cognitive_data_{SELECTED_ACTION}/`\n",
        "- **Checkpoints**: Every 250 examples\n",
        "- **Final file**: `{SELECTED_ACTION}_7k_final_[timestamp].jsonl`\n",
        "\n",
        "### To generate for another cognitive action:\n",
        "1. Change `SELECTED_ACTION` in cell 5\n",
        "2. Rerun from cell 5 onwards\n",
        "\n",
        "### Example format:\n",
        "```json\n",
        "{\n",
        "  \"text\": \"I need to analyze my monthly budget...\",\n",
        "  \"cognitive_action\": \"analyzing\",\n",
        "  \"domain\": \"finances\"\n",
        "}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
