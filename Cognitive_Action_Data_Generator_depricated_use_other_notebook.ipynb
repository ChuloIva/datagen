{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Cognitive Action Training Data Generator\n",
    "\n",
    "This notebook generates high-quality training data for cognitive action recognition using Ollama locally or via API.\n",
    "\n",
    "**Based on Scientific Taxonomies:**\n",
    "- Bloom's Taxonomy (cognitive processes)\n",
    "- Guilford's Structure of Intellect\n",
    "- Krathwohl's Affective Domain\n",
    "- Gross's Emotion Regulation Model\n",
    "- Metacognitive Process Frameworks\n",
    "\n",
    "**Target:** 100,000+ diverse examples of explicit cognitive and psychological actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": "# Install required packages\n!pip install requests pandas numpy tqdm matplotlib seaborn aiohttp nest-asyncio\n\n# Clone the repository\nimport os\nif not os.path.exists('datagen'):\n    print(\"Cloning datagen repository...\")\n    !git clone https://github.com/ChuloIva/datagen.git\n    print(\"‚úÖ Repository cloned successfully!\")\nelse:\n    print(\"‚úÖ Repository already exists\")\n\n# If running locally with Ollama installed:\n# !pip install ollama\n\nimport json\nimport time\nimport random\nimport asyncio\nimport aiohttp\nimport nest_asyncio\nimport requests\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Apply nest_asyncio to allow nested event loops (required for Jupyter/Colab)\nnest_asyncio.apply()\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\n\nprint(\"Dependencies installed successfully!\")\nprint(\"‚úì nest_asyncio applied - async operations will work in Jupyter/Colab\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_files"
   },
   "source": "## 2. Import Supporting Modules\n\nThe following Python modules are part of the repository:\n- `variable_pools.py` - Contains cognitive action taxonomies and variable pools\n- `prompt_templates.py` - Template system for generating diverse prompts\n- `data_generator.py` - Main data generation engine\n\nThese files will be imported from the cloned `datagen/` directory."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_variable_pools"
   },
   "outputs": [],
   "source": "# Import variable pools module\nimport sys\nimport os\n\n# Add the datagen directory to Python path\ndatagen_dir = os.path.abspath('datagen')\nif datagen_dir not in sys.path:\n    sys.path.insert(0, datagen_dir)\n\n# Import from the repository\nfrom variable_pools import *\n\nprint(\"Variable pools loaded successfully!\")\nprint(f\"Total cognitive actions: {len(COGNITIVE_ACTIONS)}\")\nprint(f\"Total domains: {len(DOMAINS)}\")\nprint(f\"Total subjects: {len(SUBJECTS)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_prompt_templates"
   },
   "outputs": [],
   "source": "# Import prompt templates module\nfrom prompt_templates import *\n\nprint(\"Prompt templates loaded successfully!\")\nprint(f\"Single action templates: {len(SINGLE_ACTION_TEMPLATES)}\")\nprint(f\"Chain templates: {len(CHAIN_TEMPLATES)}\")\nprint(f\"Dialogue templates: {len(DIALOGUE_TEMPLATES)}\")\nprint(f\"Thought stream templates: {len(THOUGHT_STREAM_TEMPLATES)}\")\nprint(f\"Negative templates: {len(NEGATIVE_TEMPLATES)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ollama_setup"
   },
   "source": [
    "## 3. Ollama Setup\n",
    "\n",
    "Choose one of the following options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ollama_local"
   },
   "source": [
    "### Option A: Local Ollama Installation (Recommended)\n",
    "\n",
    "If you have Ollama running locally, modify the URL below to point to your instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_ollama_local"
   },
   "outputs": [],
   "source": "# ============================================================================\n# PROPER OLLAMA SETUP FOR GOOGLE COLAB WITH HIGH PARALLELISM\n# ============================================================================\n# This must be run BEFORE starting Ollama server for the first time!\n\nimport os\nimport subprocess\nimport time\n\n# Stop any existing Ollama processes\nprint(\"üõë Stopping any existing Ollama processes...\")\nsubprocess.run(['pkill', '-9', 'ollama'], stderr=subprocess.DEVNULL)\ntime.sleep(2)\n\n# Set environment variables BEFORE starting Ollama\nprint(\"\\n‚öôÔ∏è  Configuring Ollama environment variables...\")\nos.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\nos.environ['OLLAMA_ORIGINS'] = '*'\nos.environ['OLLAMA_NUM_PARALLEL'] = '16'  # Allow 16 parallel context buffers\nos.environ['OLLAMA_MAX_QUEUE'] = '512'\nos.environ['OLLAMA_MAX_LOADED_MODELS'] = '1'\nos.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia'  # Important for GPU\n\nprint(\"Ollama configuration:\")\nfor key in ['OLLAMA_HOST', 'OLLAMA_NUM_PARALLEL', 'OLLAMA_MAX_QUEUE', 'OLLAMA_MAX_LOADED_MODELS']:\n    print(f\"  {key}: {os.environ.get(key)}\")\n\n# Start Ollama server with environment variables\nprint(\"\\nüöÄ Starting Ollama server...\")\nsubprocess.Popen(['ollama', 'serve'], \n                 env=os.environ.copy(),\n                 stdout=subprocess.DEVNULL,\n                 stderr=subprocess.DEVNULL)\n\nprint(\"‚è≥ Waiting for Ollama to start...\")\ntime.sleep(5)\n\n# Verify Ollama is running\ntry:\n    import requests\n    response = requests.get('http://localhost:11434/api/tags', timeout=5)\n    if response.status_code == 200:\n        print(\"‚úÖ Ollama is running successfully!\")\n        models = response.json().get('models', [])\n        if models:\n            print(f\"\\nüì¶ Available models: {len(models)}\")\n            for model in models:\n                print(f\"  - {model['name']}\")\n        else:\n            print(\"\\n‚ö†Ô∏è  No models loaded yet. Pull a model with: !ollama pull gemma3:27b\")\n    else:\n        print(\"‚ùå Ollama responded but with error\")\nexcept Exception as e:\n    print(f\"‚ùå Error connecting to Ollama: {e}\")\n    print(\"Try running: !ollama serve\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"IMPORTANT: Understanding VRAM Usage\")\nprint(\"=\"*60)\nprint(\"With gemma3:27b (18GB model):\")\nprint(\"  ‚Ä¢ Model weights: ~18GB (loaded once)\")\nprint(\"  ‚Ä¢ Each parallel context: ~1-2GB\")\nprint(\"  ‚Ä¢ 16 parallel = 18GB + ~16-32GB = 34-50GB total\")\nprint(\"\\nYour 18GB usage is NORMAL if:\")\nprint(\"  ‚úì Only 1 model is loaded\")\nprint(\"  ‚úì Not currently processing requests\")\nprint(\"\\nVRAM will increase to ~35-40GB when generation starts!\")"
  },
  {
   "cell_type": "code",
   "source": "# Configure Ollama for high parallelism\n# This tells Ollama to process up to 16 requests simultaneously\n# Adjust based on your VRAM (40GB can handle 12-16 parallel for gemma3:27b)\n\nimport os\n\n# Set environment variables for Ollama\nos.environ['OLLAMA_NUM_PARALLEL'] = '16'  # Max parallel requests per model\nos.environ['OLLAMA_MAX_QUEUE'] = '512'     # Max queued requests\nos.environ['OLLAMA_MAX_LOADED_MODELS'] = '1'  # Only need 1 model loaded\n\nprint(\"Ollama configuration:\")\nprint(f\"  OLLAMA_NUM_PARALLEL: {os.environ.get('OLLAMA_NUM_PARALLEL')}\")\nprint(f\"  OLLAMA_MAX_QUEUE: {os.environ.get('OLLAMA_MAX_QUEUE')}\")\nprint(f\"  OLLAMA_MAX_LOADED_MODELS: {os.environ.get('OLLAMA_MAX_LOADED_MODELS')}\")\nprint(\"\\n‚ö†Ô∏è  IMPORTANT: You must RESTART Ollama for these changes to take effect!\")\nprint(\"\\nIn Colab, restart Ollama with:\")\nprint(\"  !killall ollama\")\nprint(\"  !nohup ollama serve > ollama.log 2>&1 &\")\nprint(\"  time.sleep(5)\")\nprint(\"\\nThen verify with: !curl http://localhost:11434/api/tags\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### ‚öôÔ∏è Configure Ollama for Maximum Parallelism (IMPORTANT!)\n\n**Run this BEFORE starting generation to maximize GPU utilization:**\n\nBy default, Ollama only processes 1-4 requests at a time. We need to increase this to match our `max_parallel` setting.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### üöÄ Performance Optimization: Parallel Batch Processing\n\nThis notebook now uses **async parallel processing** for significantly faster generation!\n\n**Key Features:**\n- Uses Python's `asyncio` and `aiohttp` for concurrent requests\n- Automatically batches multiple requests to Ollama\n- Controlled by `max_parallel` parameter (default: 4)\n\n**Configuration Guide:**\n\n| Hardware Setup | Recommended `max_parallel` | Expected Speedup |\n|---------------|---------------------------|------------------|\n| CPU only, 16GB RAM | 2-4 | 2-4x |\n| CPU only, 32GB+ RAM | 4-8 | 4-8x |\n| GPU, 8-12GB VRAM | 4-8 | 4-8x |\n| GPU, 16-24GB VRAM | 8-16 | 8-16x |\n| GPU, 24GB+ VRAM | 16-32 | 16-32x |\n\n**Environment Variables (Optional):**\n```bash\n# Set these before starting Ollama for optimal performance\nexport OLLAMA_NUM_PARALLEL=8    # Max parallel requests per model\nexport OLLAMA_MAX_QUEUE=512     # Max queued requests\nexport OLLAMA_MAX_LOADED_MODELS=3  # Max models in memory\n```\n\n**Usage:**\n```python\n# Create generator with custom parallelism\ngenerator = CognitiveDataGenerator(ollama_client=ollama, max_parallel=8)\n\n# Generate batch - automatically uses parallel processing\nexamples = generator.generate_batch(batch_size=100, model=\"llama3.2\")\n```\n\nThe `delay` parameter is now ignored as async handles rate limiting automatically!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ollama_colab"
   },
   "source": [
    "### Option B: Install Ollama in Colab (Experimental)\n",
    "\n",
    "‚ö†Ô∏è **Note:** This is experimental and may not work reliably in all Colab environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_ollama_colab"
   },
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to install Ollama directly in Colab\n",
    "# This is experimental and may not work in all environments\n",
    "\n",
    "# !curl -fsSL https://ollama.ai/install.sh | sh\n",
    "# \n",
    "# # Start Ollama in background\n",
    "# import subprocess\n",
    "# import time\n",
    "# \n",
    "# # Start Ollama server\n",
    "# ollama_process = subprocess.Popen(['ollama', 'serve'], \n",
    "#                                   stdout=subprocess.PIPE, \n",
    "#                                   stderr=subprocess.PIPE)\n",
    "# \n",
    "# # Wait for server to start\n",
    "# time.sleep(10)\n",
    "# \n",
    "# # Pull a model\n",
    "# !ollama pull llama3.2\n",
    "# \n",
    "# print(\"Ollama installed and model pulled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_generation"
   },
   "source": [
    "## 4. Data Generation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_data_generator"
   },
   "outputs": [],
   "source": "# Import data generator module\nfrom data_generator import *\n\nprint(\"Data generator loaded successfully!\")\nprint(\"CognitiveDataGenerator class ready to use\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_modules"
   },
   "outputs": [],
   "source": "# Verify all modules are imported and working\nprint(\"All modules imported successfully!\")\nprint(f\"Ready to generate data with {len(COGNITIVE_ACTIONS)} cognitive actions\")\nprint(f\"\\nAvailable components:\")\nprint(f\"  - Variable pools: ‚úì\")\nprint(f\"  - Prompt templates: ‚úì\")\nprint(f\"  - Data generator: ‚úì\")\nprint(f\"  - Ollama client: {'‚úì' if 'ollama' in dir() else '‚ö†Ô∏è Configure in section 3'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_generation"
   },
   "source": [
    "## 5. Test the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_prompt_generation"
   },
   "outputs": [],
   "source": "# Test with Ollama (if connected)\n# Configure max_parallel to control concurrency (default: 4, increase if you have more VRAM/RAM)\ngenerator = CognitiveDataGenerator(ollama_client=ollama, max_parallel=4)\n\nprint(\"Testing single example generation...\")\n\n# Generate a single example\nexample = generator.generate_single_example(\n    cognitive_action=\"reconsidering\",\n    template_type=\"single\",\n    model=\"gemma3:27b\"  # Default model - change if you have a different model\n)\n\nif example:\n    print(\"\\n=== GENERATED EXAMPLE ===\")\n    print(f\"Cognitive Action: {example.primary_cognitive_action}\")\n    print(f\"Domain: {example.domain}\")\n    print(f\"Complexity: {example.complexity}\")\n    print(f\"Format: {example.format_type}\")\n    print()\n    print(\"Generated Text:\")\n    print(example.text)\n    print()\n    print(\"Metadata:\")\n    for key, value in example.metadata.items():\n        if key != 'prompt_used':  # Skip the long prompt\n            print(f\"  {key}: {value}\")\nelse:\n    print(\"Failed to generate example. Check Ollama connection.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_with_ollama"
   },
   "outputs": [],
   "source": "# Test with Ollama (if connected)\n# Configure max_parallel to control concurrency (default: 4, increase if you have more VRAM/RAM)\ngenerator = CognitiveDataGenerator(ollama_client=ollama, max_parallel=4)\n\nprint(\"Testing single example generation...\")\n\n# Generate a single example\nexample = generator.generate_single_example(\n    cognitive_action=\"reconsidering\",\n    template_type=\"single\",\n    model=\"llama3.2\"  # Change this to your available model\n)\n\nif example:\n    print(\"\\n=== GENERATED EXAMPLE ===\")\n    print(f\"Cognitive Action: {example.primary_cognitive_action}\")\n    print(f\"Domain: {example.domain}\")\n    print(f\"Complexity: {example.complexity}\")\n    print(f\"Format: {example.format_type}\")\n    print()\n    print(\"Generated Text:\")\n    print(example.text)\n    print()\n    print(\"Metadata:\")\n    for key, value in example.metadata.items():\n        if key != 'prompt_used':  # Skip the long prompt\n            print(f\"  {key}: {value}\")\nelse:\n    print(\"Failed to generate example. Check Ollama connection.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_generation"
   },
   "source": "# Generate a small batch for testing with parallel processing\nprint(\"Generating small test batch using async parallel processing...\")\nprint(\"This will be significantly faster than sequential generation!\")\n\ntest_examples = generator.generate_batch(\n    batch_size=10,\n    cognitive_action=None,  # Random actions\n    template_type=\"single\",\n    model=\"gemma3:27b\"  # Default model\n    # Note: 'delay' parameter is now ignored as we use async parallel processing\n)\n\nprint(f\"\\n‚úì Generated {len(test_examples)} examples\")\n\n# Show a few examples\nfor i, example in enumerate(test_examples[:3]):\n    print(f\"\\n=== EXAMPLE {i+1} ===\")\n    print(f\"Action: {example.primary_cognitive_action}\")\n    print(f\"Domain: {example.domain}\")\n    print(f\"Text: {example.text[:200]}...\")\n\n# Show statistics\ngenerator.print_statistics()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "small_batch_test"
   },
   "outputs": [],
   "source": "# Generate stratified dataset (smaller for testing) using parallel processing\nprint(\"Generating stratified test dataset with parallel processing...\")\nprint(\"Note: Stratified generation uses multiple batch calls internally\")\n\n# Create new generator for clean stats with higher parallelism\n# Increase max_parallel if you have sufficient VRAM/RAM (recommended: 8-16 for GPUs with 24GB+ VRAM)\nstratified_generator = CognitiveDataGenerator(ollama_client=ollama, max_parallel=8)\n\nstratified_examples = stratified_generator.generate_stratified_dataset(\n    total_examples=100,  # Start small for testing\n    model=\"gemma3:27b\"  # Default model\n)\n\nprint(f\"\\n‚úì Generated {len(stratified_examples)} stratified examples\")\nstratified_generator.print_statistics()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stratified_generation"
   },
   "outputs": [],
   "source": "# Generate stratified dataset (smaller for testing) using parallel processing\nprint(\"Generating stratified test dataset with parallel processing...\")\nprint(\"Note: Stratified generation uses multiple batch calls internally\")\n\n# Create new generator for clean stats with higher parallelism\n# Increase max_parallel if you have sufficient VRAM/RAM (recommended: 8-16 for GPUs with 24GB+ VRAM)\nstratified_generator = CognitiveDataGenerator(ollama_client=ollama, max_parallel=8)\n\nstratified_examples = stratified_generator.generate_stratified_dataset(\n    total_examples=100,  # Start small for testing\n    model=\"llama3.2\"\n)\n\nprint(f\"\\n‚úì Generated {len(stratified_examples)} stratified examples\")\nstratified_generator.print_statistics()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_analysis"
   },
   "source": [
    "## 7. Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze_data"
   },
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "data = []\n",
    "for example in stratified_generator.generated_examples:\n",
    "    data.append({\n",
    "        'text': example.text,\n",
    "        'cognitive_action': example.primary_cognitive_action,\n",
    "        'domain': example.domain,\n",
    "        'complexity': example.complexity,\n",
    "        'format_type': example.format_type,\n",
    "        'text_length': len(example.text),\n",
    "        'word_count': len(example.text.split()),\n",
    "        'subject': example.metadata.get('subject', ''),\n",
    "        'emotional_state': example.metadata.get('emotional_state', '')\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_data"
   },
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Distribution of cognitive actions\n",
    "plt.subplot(2, 3, 1)\n",
    "cognitive_action_counts = df['cognitive_action'].value_counts()\n",
    "plt.bar(range(len(cognitive_action_counts)), cognitive_action_counts.values)\n",
    "plt.xticks(range(len(cognitive_action_counts)), cognitive_action_counts.index, rotation=45, ha='right')\n",
    "plt.title('Distribution of Cognitive Actions')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Distribution of domains\n",
    "plt.subplot(2, 3, 2)\n",
    "domain_counts = df['domain'].value_counts()[:10]  # Top 10\n",
    "plt.bar(range(len(domain_counts)), domain_counts.values)\n",
    "plt.xticks(range(len(domain_counts)), domain_counts.index, rotation=45, ha='right')\n",
    "plt.title('Top 10 Domains')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Distribution of complexity levels\n",
    "plt.subplot(2, 3, 3)\n",
    "complexity_counts = df['complexity'].value_counts()\n",
    "plt.pie(complexity_counts.values, labels=complexity_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Complexity Distribution')\n",
    "\n",
    "# Text length distribution\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.hist(df['text_length'], bins=20, edgecolor='black')\n",
    "plt.title('Text Length Distribution')\n",
    "plt.xlabel('Characters')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Word count distribution\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.hist(df['word_count'], bins=20, edgecolor='black')\n",
    "plt.title('Word Count Distribution')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Format type distribution\n",
    "plt.subplot(2, 3, 6)\n",
    "format_counts = df['format_type'].value_counts()\n",
    "plt.pie(format_counts.values, labels=format_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Format Type Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show some sample texts by cognitive action\n",
    "print(\"\\n=== SAMPLE TEXTS BY COGNITIVE ACTION ===\")\n",
    "for action in df['cognitive_action'].unique()[:5]:  # First 5 actions\n",
    "    sample = df[df['cognitive_action'] == action]['text'].iloc[0]\n",
    "    print(f\"\\n{action.upper()}:\")\n",
    "    print(f\"  {sample[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export_data"
   },
   "source": "# Configuration for large-scale generation with parallel processing\n# Target: 7,000 examples total (stratified across all cognitive actions)\n# Optimized to fit within 5-7 hours of generation time\nLARGE_SCALE_CONFIG = {\n    'total_examples': 7000,   # Total target examples\n    'model': 'gemma3:27b',    # Default model\n    'max_parallel': 16,       # Parallel requests (uses 16 context buffers)\n    'checkpoint_interval': 100,  # Save progress every 100 examples\n    'checkpoint_dir': '/content/drive/MyDrive/cognitive_data_checkpoints'  # Google Drive location\n}\n\nprint(\"Large scale configuration (with parallel processing):\")\nprint(f\"Total target examples: {LARGE_SCALE_CONFIG['total_examples']:,}\")\nprint(f\"Checkpoint interval: Every {LARGE_SCALE_CONFIG['checkpoint_interval']} examples\")\nprint(f\"Checkpoint directory: {LARGE_SCALE_CONFIG['checkpoint_dir']}\")\nprint(f\"Parallel requests: {LARGE_SCALE_CONFIG['max_parallel']} concurrent context buffers\")\n\n# Calculate stratified distribution\nfrom variable_pools import COGNITIVE_ACTIONS\nexamples_per_action = LARGE_SCALE_CONFIG['total_examples'] // len(COGNITIVE_ACTIONS)\nprint(f\"\\nStratified distribution:\")\nprint(f\"  Cognitive actions: {len(COGNITIVE_ACTIONS)}\")\nprint(f\"  Examples per action: {examples_per_action:,}\")\n\n# With parallel processing, estimate time is significantly reduced\n# Assuming 30 seconds per request average\nestimated_seconds = LARGE_SCALE_CONFIG['total_examples'] / LARGE_SCALE_CONFIG['max_parallel'] * 30\nestimated_hours = estimated_seconds / 3600\nprint(f\"\\nEstimated time (with {LARGE_SCALE_CONFIG['max_parallel']} parallel requests @ 30s each):\")\nprint(f\"  Total: {estimated_hours:.1f} hours ({estimated_hours*60:.0f} minutes)\")\nprint(f\"  vs sequential: {LARGE_SCALE_CONFIG['total_examples'] * 30 / 3600:.1f} hours - {LARGE_SCALE_CONFIG['max_parallel']}x speedup!\")\n\nprint(\"\\nTemplate distribution per action:\")\nprint(f\"  70% single action examples ({int(examples_per_action * 0.7)})\")\nprint(f\"  20% chain/combination examples ({int(examples_per_action * 0.2)})\")\nprint(f\"  5% dialogue examples ({int(examples_per_action * 0.05)})\")\nprint(f\"  5% thought-stream examples ({int(examples_per_action * 0.05)})\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"VRAM Usage Expectations for 40GB GPU:\")\nprint(\"=\"*60)\nprint(f\"When IDLE (model loaded, no active requests):\")\nprint(f\"  ‚Ä¢ Model weights: ~18GB\")\nprint(f\"  ‚Ä¢ Current VRAM: ~18GB ‚úì NORMAL\")\nprint(f\"\\nDuring GENERATION ({LARGE_SCALE_CONFIG['max_parallel']} parallel requests):\")\nprint(f\"  ‚Ä¢ Model weights: ~18GB\")\nprint(f\"  ‚Ä¢ {LARGE_SCALE_CONFIG['max_parallel']} context buffers: ~{LARGE_SCALE_CONFIG['max_parallel'] * 1.5:.0f}GB\")\nprint(f\"  ‚Ä¢ Processing overhead: ~2GB\")\nestimated_vram = 18 + LARGE_SCALE_CONFIG['max_parallel'] * 1.5 + 2\nprint(f\"  ‚Ä¢ Total during generation: ~{estimated_vram:.0f}GB\")\nprint(f\"  ‚Ä¢ Safety buffer: ~{40 - estimated_vram:.0f}GB\")\nprint(\"\\nüí° VRAM will jump from 18GB ‚Üí ~38GB when you start generation!\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_datasets"
   },
   "outputs": [],
   "source": [
    "# Export the generated dataset\n",
    "timestamp = int(time.time())\n",
    "\n",
    "# Export as JSONL (recommended for large datasets)\n",
    "jsonl_filename = f\"cognitive_actions_dataset_{timestamp}.jsonl\"\n",
    "stratified_generator.export_dataset(jsonl_filename, format=\"jsonl\")\n",
    "\n",
    "# Export as JSON (for smaller datasets)\n",
    "json_filename = f\"cognitive_actions_dataset_{timestamp}.json\"\n",
    "stratified_generator.export_dataset(json_filename, format=\"json\")\n",
    "\n",
    "# Export as CSV for analysis\n",
    "csv_filename = f\"cognitive_actions_analysis_{timestamp}.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"\\nDataset exported:\")\n",
    "print(f\"  JSONL: {jsonl_filename}\")\n",
    "print(f\"  JSON: {json_filename}\")\n",
    "print(f\"  CSV: {csv_filename}\")\n",
    "\n",
    "# Show file sizes\n",
    "import os\n",
    "for filename in [jsonl_filename, json_filename, csv_filename]:\n",
    "    if os.path.exists(filename):\n",
    "        size = os.path.getsize(filename)\n",
    "        print(f\"  {filename}: {size:,} bytes ({size/1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "large_scale_generation"
   },
   "source": "# Configuration for large-scale generation with parallel processing\n# Target: 7,000 examples total (stratified across all cognitive actions)\n# Optimized to fit within 7-9 hours of generation time\nLARGE_SCALE_CONFIG = {\n    'total_examples': 7000,   # Total target examples (fits in ~7.3 hours @ 30s/request)\n    'model': 'gemma3:27b',    # Default model\n    'max_parallel': 6,        # Parallel requests (optimized for 40GB VRAM with 18GB model)\n    'checkpoint_interval': 100,  # Save progress every 100 examples\n    'checkpoint_dir': '/content/drive/MyDrive/cognitive_data_checkpoints'  # Google Drive location\n}\n\nprint(\"Large scale configuration (with parallel processing):\")\nprint(f\"Total target examples: {LARGE_SCALE_CONFIG['total_examples']:,}\")\nprint(f\"Checkpoint interval: Every {LARGE_SCALE_CONFIG['checkpoint_interval']} examples\")\nprint(f\"Checkpoint directory: {LARGE_SCALE_CONFIG['checkpoint_dir']}\")\nprint(f\"Parallel requests: {LARGE_SCALE_CONFIG['max_parallel']} (optimized for 40GB VRAM)\")\n\n# Calculate stratified distribution\nfrom variable_pools import COGNITIVE_ACTIONS\nexamples_per_action = LARGE_SCALE_CONFIG['total_examples'] // len(COGNITIVE_ACTIONS)\nprint(f\"\\nStratified distribution:\")\nprint(f\"  Cognitive actions: {len(COGNITIVE_ACTIONS)}\")\nprint(f\"  Examples per action: {examples_per_action:,}\")\n\n# With parallel processing, estimate time is significantly reduced\n# Assuming 30 seconds per request average\nestimated_seconds = LARGE_SCALE_CONFIG['total_examples'] / LARGE_SCALE_CONFIG['max_parallel'] * 30\nestimated_hours = estimated_seconds / 3600\nprint(f\"\\nEstimated time (with {LARGE_SCALE_CONFIG['max_parallel']} parallel requests @ 30s each):\")\nprint(f\"  Total: {estimated_hours:.1f} hours ({estimated_hours*60:.0f} minutes)\")\nprint(f\"  vs sequential: {LARGE_SCALE_CONFIG['total_examples'] * 30 / 3600:.1f} hours - {LARGE_SCALE_CONFIG['max_parallel']}x speedup!\")\n\nprint(\"\\nTemplate distribution per action:\")\nprint(f\"  70% single action examples ({int(examples_per_action * 0.7)})\")\nprint(f\"  20% chain/combination examples ({int(examples_per_action * 0.2)})\")\nprint(f\"  5% dialogue examples ({int(examples_per_action * 0.05)})\")\nprint(f\"  5% thought-stream examples ({int(examples_per_action * 0.05)})\")\n\nprint(\"\\nVRAM estimation for 40GB GPU:\")\nprint(f\"  Model size: ~18GB\")\nprint(f\"  {LARGE_SCALE_CONFIG['max_parallel']} parallel requests (KV cache): ~{LARGE_SCALE_CONFIG['max_parallel'] * 2}GB\")\nprint(f\"  Processing overhead: ~2GB\")\nprint(f\"  Total estimated: ~{18 + LARGE_SCALE_CONFIG['max_parallel'] * 2 + 2}GB\")\nprint(f\"  Safety buffer: ~{40 - (18 + LARGE_SCALE_CONFIG['max_parallel'] * 2 + 2)}GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configure_large_scale"
   },
   "outputs": [],
   "source": "# Uncomment and run this cell for large-scale stratified generation with parallel processing\n# Target: 7,000 examples with checkpoints every 100 examples (fits in 7-9 hours)\n# Uses stratified sampling to ensure all cognitive actions are represented evenly\n\n# def run_large_scale_generation():\n#     \"\"\"Run stratified large-scale generation pipeline with parallel processing and checkpointing\"\"\"\n#     \n#     # Mount Google Drive for checkpoint storage (Colab only)\n#     try:\n#         from google.colab import drive\n#         drive.mount('/content/drive')\n#         print(\"‚úì Google Drive mounted successfully\")\n#     except:\n#         print(\"‚ö†Ô∏è  Not running in Colab - checkpoints will be saved locally\")\n#         LARGE_SCALE_CONFIG['checkpoint_dir'] = './checkpoints'\n#     \n#     # Create checkpoint directory\n#     import os\n#     os.makedirs(LARGE_SCALE_CONFIG['checkpoint_dir'], exist_ok=True)\n#     print(f\"‚úì Checkpoint directory created: {LARGE_SCALE_CONFIG['checkpoint_dir']}\")\n#     \n#     # Initialize generator with parallel processing\n#     large_generator = CognitiveDataGenerator(\n#         ollama_client=ollama,\n#         max_parallel=LARGE_SCALE_CONFIG['max_parallel']\n#     )\n#     \n#     # Calculate examples per cognitive action for stratification\n#     total_target = LARGE_SCALE_CONFIG['total_examples']\n#     examples_per_action = total_target // len(COGNITIVE_ACTIONS)\n#     checkpoint_interval = LARGE_SCALE_CONFIG['checkpoint_interval']\n#     \n#     print(f\"\\n{'='*60}\")\n#     print(f\"üöÄ Starting Stratified Generation\")\n#     print(f\"Total target: {total_target:,} examples\")\n#     print(f\"Cognitive actions: {len(COGNITIVE_ACTIONS)}\")\n#     print(f\"Examples per action: {examples_per_action:,}\")\n#     print(f\"Using {LARGE_SCALE_CONFIG['max_parallel']} parallel requests\")\n#     print(f\"Checkpoints every {checkpoint_interval} examples\")\n#     print(f\"{'='*60}\\n\")\n#     \n#     generation_start_time = time.time()\n#     checkpoint_counter = 0\n#     \n#     # Generate stratified dataset by cognitive action\n#     for action_idx, action in enumerate(COGNITIVE_ACTIONS.keys(), 1):\n#         print(f\"\\n[{action_idx}/{len(COGNITIVE_ACTIONS)}] Generating {examples_per_action:,} examples for: {action}\")\n#         \n#         # Mix template types for variety (70% single, 20% chain, 10% other)\n#         template_distribution = (\n#             [\"single\"] * int(examples_per_action * 0.7) +\n#             [\"chain\"] * int(examples_per_action * 0.2) +\n#             [\"dialogue\"] * int(examples_per_action * 0.05) +\n#             [\"thought_stream\"] * int(examples_per_action * 0.05)\n#         )\n#         # Adjust to exact count\n#         while len(template_distribution) < examples_per_action:\n#             template_distribution.append(\"single\")\n#         template_distribution = template_distribution[:examples_per_action]\n#         random.shuffle(template_distribution)\n#         \n#         # Generate in checkpoint-sized batches\n#         num_batches = (examples_per_action + checkpoint_interval - 1) // checkpoint_interval\n#         \n#         for batch_idx in range(num_batches):\n#             start_idx = batch_idx * checkpoint_interval\n#             end_idx = min(start_idx + checkpoint_interval, examples_per_action)\n#             batch_templates = template_distribution[start_idx:end_idx]\n#             \n#             # Count template types in this batch\n#             template_counts = {}\n#             for template in batch_templates:\n#                 template_counts[template] = template_counts.get(template, 0) + 1\n#             \n#             # Generate for each template type in batch\n#             batch_examples = []\n#             for template_type, count in template_counts.items():\n#                 print(f\"  Batch {batch_idx + 1}/{num_batches}: Generating {count} {template_type} examples...\")\n#                 examples = large_generator.generate_batch(\n#                     batch_size=count,\n#                     cognitive_action=action,\n#                     template_type=template_type,\n#                     model=LARGE_SCALE_CONFIG['model']\n#                 )\n#                 batch_examples.extend(examples)\n#             \n#             # Save checkpoint\n#             checkpoint_counter += 1\n#             checkpoint_filename = os.path.join(\n#                 LARGE_SCALE_CONFIG['checkpoint_dir'],\n#                 f\"checkpoint_{checkpoint_counter:04d}_{action}_{int(time.time())}.jsonl\"\n#             )\n#             \n#             with open(checkpoint_filename, 'w') as f:\n#                 for example in batch_examples:\n#                     json_obj = {\n#                         'text': example.text,\n#                         'primary_cognitive_action': example.primary_cognitive_action,\n#                         'secondary_actions': example.secondary_actions,\n#                         'domain': example.domain,\n#                         'complexity': example.complexity,\n#                         'perspective': example.perspective,\n#                         'format_type': example.format_type,\n#                         'metadata': example.metadata\n#                     }\n#                     f.write(json.dumps(json_obj) + '\\n')\n#             \n#             print(f\"  ‚úì Checkpoint {checkpoint_counter} saved: {os.path.basename(checkpoint_filename)}\")\n#             print(f\"  Total progress: {len(large_generator.generated_examples):,}/{total_target:,} ({len(large_generator.generated_examples)/total_target*100:.1f}%)\\n\")\n#     \n#     # Final export\n#     elapsed_time = time.time() - generation_start_time\n#     final_timestamp = int(time.time())\n#     final_filename = os.path.join(\n#         LARGE_SCALE_CONFIG['checkpoint_dir'],\n#         f\"cognitive_actions_stratified_7k_{final_timestamp}.jsonl\"\n#     )\n#     large_generator.export_dataset(final_filename, format=\"jsonl\")\n#     \n#     print(f\"\\n{'='*60}\")\n#     print(f\"üéâ COMPLETE! Generated {len(large_generator.generated_examples):,} examples\")\n#     print(f\"‚è±Ô∏è  Total time: {elapsed_time/3600:.2f} hours ({elapsed_time/60:.1f} minutes)\")\n#     print(f\"üìÅ Final dataset: {final_filename}\")\n#     print(f\"üìä Total checkpoints: {checkpoint_counter}\")\n#     print(f\"üìä Total errors: {len(large_generator.generation_stats['errors'])}\")\n#     print(f\"{'='*60}\\n\")\n#     \n#     # Print final statistics\n#     large_generator.print_statistics()\n#     \n#     return large_generator\n\n# # Uncomment the line below to start large-scale generation\n# # large_generator = run_large_scale_generation()\n\nprint(\"Stratified large-scale generation function defined with parallel processing.\")\nprint(f\"Target: {LARGE_SCALE_CONFIG['total_examples']:,} examples with checkpoints every {LARGE_SCALE_CONFIG['checkpoint_interval']} examples\")\nprint(f\"Distribution: ~{LARGE_SCALE_CONFIG['total_examples'] // len(COGNITIVE_ACTIONS):,} examples per cognitive action\")\nprint(\"\\nTo start generation:\")\nprint(\"1. Uncomment the last line: large_generator = run_large_scale_generation()\")\nprint(\"2. Run this cell\")\nprint(\"3. Checkpoints will be saved to Google Drive (if running in Colab)\")\nestimated_hours = LARGE_SCALE_CONFIG['total_examples'] / LARGE_SCALE_CONFIG['max_parallel'] * 30 / 3600\nprint(f\"\\n‚è±Ô∏è  Estimated time: ~{estimated_hours:.1f} hours ({estimated_hours*60:.0f} minutes) @ 30s per request\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_large_scale"
   },
   "outputs": [],
   "source": "# Uncomment and run this cell for large-scale generation with parallel processing\n# WARNING: This will take a very long time, but much faster than sequential!\n\n# def run_large_scale_generation():\n#     \"\"\"Run the complete large-scale generation pipeline with parallel processing\"\"\"\n#     \n#     # Initialize generator with parallel processing\n#     large_generator = CognitiveDataGenerator(\n#         ollama_client=ollama,\n#         max_parallel=LARGE_SCALE_CONFIG['max_parallel']\n#     )\n#     \n#     phases = [\n#         ('phase1_round1', 'single', LARGE_SCALE_CONFIG['phase1_round1']),\n#         ('phase1_round2', 'chain', LARGE_SCALE_CONFIG['phase1_round2']),\n#         ('phase2', 'single', LARGE_SCALE_CONFIG['phase2']),\n#         ('phase3', 'single', LARGE_SCALE_CONFIG['phase3']),\n#         ('phase4', 'negative', LARGE_SCALE_CONFIG['phase4']),\n#         ('phase5', 'dialogue', LARGE_SCALE_CONFIG['phase5']),\n#         ('phase6', 'thought_stream', LARGE_SCALE_CONFIG['phase6'])\n#     ]\n#     \n#     for phase_name, template_type, target_count in phases:\n#         print(f\"\\nüöÄ Starting {phase_name}: {target_count:,} examples\")\n#         print(f\"Template type: {template_type}\")\n#         print(f\"Using {LARGE_SCALE_CONFIG['max_parallel']} parallel requests\")\n#         \n#         phase_examples = large_generator.generate_batch(\n#             batch_size=target_count,\n#             template_type=template_type,\n#             model=LARGE_SCALE_CONFIG['model']\n#             # No delay needed with async parallel processing!\n#         )\n#         \n#         print(f\"‚úÖ Completed {phase_name}: {len(phase_examples):,} examples\")\n#         \n#         # Checkpoint save\n#         checkpoint_filename = f\"checkpoint_{phase_name}_{int(time.time())}.jsonl\"\n#         large_generator.export_dataset(checkpoint_filename, format=\"jsonl\")\n#         print(f\"üíæ Checkpoint saved: {checkpoint_filename}\")\n#         \n#         # Print progress statistics\n#         large_generator.print_statistics()\n#     \n#     # Final export\n#     final_timestamp = int(time.time())\n#     final_filename = f\"cognitive_actions_complete_dataset_{final_timestamp}.jsonl\"\n#     large_generator.export_dataset(final_filename, format=\"jsonl\")\n#     \n#     print(f\"\\nüéâ COMPLETE! Generated {len(large_generator.generated_examples):,} examples\")\n#     print(f\"üìÅ Final dataset: {final_filename}\")\n#     \n#     return large_generator\n\n# # Uncomment the line below to start large-scale generation\n# # large_generator = run_large_scale_generation()\n\nprint(\"Large-scale generation function defined with parallel processing.\")\nprint(\"Uncomment the last line and run this cell to start large-scale generation.\")\nprint(f\"‚ö†Ô∏è  With {LARGE_SCALE_CONFIG['max_parallel']}x parallel processing, this will be {LARGE_SCALE_CONFIG['max_parallel']}x faster!\")\nprint(\"Make sure you have sufficient time and resources before starting!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quality_control"
   },
   "source": [
    "## 10. Quality Control and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quality_checks"
   },
   "outputs": [],
   "source": [
    "def quality_control_analysis(examples):\n",
    "    \"\"\"Perform quality control analysis on generated examples\"\"\"\n",
    "    \n",
    "    print(\"=== QUALITY CONTROL ANALYSIS ===\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_examples = len(examples)\n",
    "    print(f\"Total examples analyzed: {total_examples:,}\")\n",
    "    \n",
    "    # Text length analysis\n",
    "    text_lengths = [len(ex.text) for ex in examples]\n",
    "    word_counts = [len(ex.text.split()) for ex in examples]\n",
    "    \n",
    "    print(f\"\\nText Length Statistics:\")\n",
    "    print(f\"  Average characters: {np.mean(text_lengths):.1f}\")\n",
    "    print(f\"  Average words: {np.mean(word_counts):.1f}\")\n",
    "    print(f\"  Min/Max characters: {min(text_lengths)} / {max(text_lengths)}\")\n",
    "    print(f\"  Min/Max words: {min(word_counts)} / {max(word_counts)}\")\n",
    "    \n",
    "    # Coverage analysis\n",
    "    cognitive_actions = set(ex.primary_cognitive_action for ex in examples)\n",
    "    domains = set(ex.domain for ex in examples)\n",
    "    \n",
    "    print(f\"\\nCoverage Statistics:\")\n",
    "    print(f\"  Cognitive actions covered: {len(cognitive_actions)}/{len(COGNITIVE_ACTIONS)} ({len(cognitive_actions)/len(COGNITIVE_ACTIONS)*100:.1f}%)\")\n",
    "    print(f\"  Domains covered: {len(domains)}/{len(DOMAINS)} ({len(domains)/len(DOMAINS)*100:.1f}%)\")\n",
    "    \n",
    "    # Quality indicators\n",
    "    print(f\"\\nQuality Indicators:\")\n",
    "    \n",
    "    # Check for very short examples\n",
    "    very_short = sum(1 for length in text_lengths if length < 50)\n",
    "    print(f\"  Very short examples (<50 chars): {very_short} ({very_short/total_examples*100:.1f}%)\")\n",
    "    \n",
    "    # Check for very long examples\n",
    "    very_long = sum(1 for length in text_lengths if length > 1000)\n",
    "    print(f\"  Very long examples (>1000 chars): {very_long} ({very_long/total_examples*100:.1f}%)\")\n",
    "    \n",
    "    # Check for examples that might contain prompt artifacts\n",
    "    prompt_artifacts = sum(1 for ex in examples if any(word in ex.text.lower() for word in \n",
    "                          ['generate', 'example', 'sentence', 'requirement', 'output']))\n",
    "    print(f\"  Potential prompt artifacts: {prompt_artifacts} ({prompt_artifacts/total_examples*100:.1f}%)\")\n",
    "    \n",
    "    # Diversity check - look for repeated phrases\n",
    "    all_texts = [ex.text for ex in examples]\n",
    "    unique_texts = set(all_texts)\n",
    "    print(f\"  Unique texts: {len(unique_texts)}/{total_examples} ({len(unique_texts)/total_examples*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'total_examples': total_examples,\n",
    "        'avg_length': np.mean(text_lengths),\n",
    "        'avg_words': np.mean(word_counts),\n",
    "        'cognitive_actions_covered': len(cognitive_actions),\n",
    "        'domains_covered': len(domains),\n",
    "        'very_short_pct': very_short/total_examples*100,\n",
    "        'very_long_pct': very_long/total_examples*100,\n",
    "        'prompt_artifacts_pct': prompt_artifacts/total_examples*100,\n",
    "        'uniqueness_pct': len(unique_texts)/total_examples*100\n",
    "    }\n",
    "\n",
    "# Run quality control on current examples\n",
    "if stratified_generator.generated_examples:\n",
    "    qc_results = quality_control_analysis(stratified_generator.generated_examples)\n",
    "else:\n",
    "    print(\"No examples to analyze. Generate some examples first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sample_review"
   },
   "outputs": [],
   "source": [
    "# Manual review of sample examples\n",
    "def review_samples(examples, n_samples=5):\n",
    "    \"\"\"Review a random sample of examples for quality\"\"\"\n",
    "    \n",
    "    if not examples:\n",
    "        print(\"No examples to review.\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== MANUAL QUALITY REVIEW ===\")\n",
    "    print(f\"Reviewing {min(n_samples, len(examples))} random examples:\\n\")\n",
    "    \n",
    "    sample_examples = random.sample(examples, min(n_samples, len(examples)))\n",
    "    \n",
    "    for i, example in enumerate(sample_examples, 1):\n",
    "        print(f\"--- EXAMPLE {i} ---\")\n",
    "        print(f\"Cognitive Action: {example.primary_cognitive_action}\")\n",
    "        print(f\"Domain: {example.domain}\")\n",
    "        print(f\"Complexity: {example.complexity}\")\n",
    "        print(f\"Format: {example.format_type}\")\n",
    "        print(f\"Length: {len(example.text)} chars, {len(example.text.split())} words\")\n",
    "        print()\n",
    "        print(\"Text:\")\n",
    "        print(f\"\\\"{example.text}\\\"\")\n",
    "        print()\n",
    "        print(\"Subject:\", example.metadata.get('subject', 'N/A'))\n",
    "        print(\"Emotional State:\", example.metadata.get('emotional_state', 'N/A'))\n",
    "        print(\"Unique Angle:\", example.metadata.get('unique_angle', 'N/A'))\n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Review samples\n",
    "if stratified_generator.generated_examples:\n",
    "    review_samples(stratified_generator.generated_examples, n_samples=3)\n",
    "else:\n",
    "    print(\"No examples to review. Generate some examples first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "You now have a complete cognitive action data generation system!\n",
    "\n",
    "### What You've Built:\n",
    "- **Scientific Foundation**: Based on established taxonomies from cognitive psychology\n",
    "- **Flexible Architecture**: Modular system with variable pools and templates\n",
    "- **Scalable Generation**: Can generate from small batches to 100,000+ examples\n",
    "- **Quality Control**: Built-in analysis and validation tools\n",
    "- **Multiple Formats**: Support for various example types (single actions, chains, dialogues, thought streams)\n",
    "\n",
    "### Next Steps:\n",
    "1. **Scale Up**: Use the large-scale generation for your full dataset\n",
    "2. **Fine-tune**: Adjust templates and variables based on your specific needs\n",
    "3. **Validate**: Run quality control on larger datasets\n",
    "4. **Train Models**: Use the generated data to train your cognitive action recognition models\n",
    "\n",
    "### Tips for Production Use:\n",
    "- Monitor generation quality regularly\n",
    "- Save checkpoints during long generation runs\n",
    "- Experiment with different Ollama models for variety\n",
    "- Consider post-processing for consistency\n",
    "\n",
    "**Happy data generating! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}