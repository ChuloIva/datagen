{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified Batch Cognitive Action Data Generator\n",
    "\n",
    "Generate any number of examples with **consistent stratified distribution** across all 45 cognitive actions.\n",
    "\n",
    "**Features:**\n",
    "- üéØ Stratified sampling: equal examples per cognitive action\n",
    "- üìä Generate any batch size (100, 1000, 10000, etc.)\n",
    "- üë§ First-person perspective only\n",
    "- üìù Simple complexity examples\n",
    "- üé® Rich variation: domains, triggers, emotional states, language styles, sentence starters\n",
    "- üíæ Auto-checkpointing and resumable\n",
    "- üöÄ Optimized for 16GB VRAM (uses gemma2:9b)\n",
    "- ‚ö° 8 concurrent requests for speed\n",
    "\n",
    "**Example:** Request 4,500 examples ‚Üí Get 100 examples per action (45 √ó 100 = 4,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q requests pandas numpy tqdm aiohttp nest-asyncio\n",
    "\n",
    "# Clone the repository\n",
    "import os\n",
    "if not os.path.exists('datagen'):\n",
    "    print(\"üì• Cloning datagen repository...\")\n",
    "    !git clone https://github.com/ChuloIva/datagen.git\n",
    "    print(\"‚úÖ Repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"‚úÖ Repository already exists\")\n",
    "\n",
    "# Import libraries\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "import requests\n",
    "import subprocess\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "# Apply nest_asyncio for Jupyter/Colab compatibility\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Install & Configure Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama\n",
    "!curl -fsSL https://ollama.ai/install.sh | sh\n",
    "\n",
    "# Stop any existing Ollama processes\n",
    "print(\"üõë Stopping any existing Ollama processes...\")\n",
    "subprocess.run(['pkill', '-9', 'ollama'], stderr=subprocess.DEVNULL)\n",
    "time.sleep(2)\n",
    "\n",
    "# Set environment variables for 16GB VRAM\n",
    "print(\"\\n‚öôÔ∏è  Configuring Ollama for 16GB VRAM...\")\n",
    "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "os.environ['OLLAMA_ORIGINS'] = '*'\n",
    "os.environ['OLLAMA_NUM_PARALLEL'] = '8'\n",
    "os.environ['OLLAMA_MAX_QUEUE'] = '256'\n",
    "os.environ['OLLAMA_MAX_LOADED_MODELS'] = '1'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia'\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: gemma2:9b (~5GB VRAM)\")\n",
    "print(f\"  Parallel requests: 8\")\n",
    "print(f\"  Expected VRAM: 12-14GB\")\n",
    "\n",
    "# Start Ollama server\n",
    "print(\"\\nüöÄ Starting Ollama server...\")\n",
    "subprocess.Popen(['ollama', 'serve'], \n",
    "                 env=os.environ.copy(),\n",
    "                 stdout=subprocess.DEVNULL,\n",
    "                 stderr=subprocess.DEVNULL)\n",
    "\n",
    "print(\"‚è≥ Waiting for Ollama to start...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Verify Ollama is running\n",
    "try:\n",
    "    response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"‚úÖ Ollama is running!\")\n",
    "    else:\n",
    "        print(\"‚ùå Ollama error\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Pull the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• Pulling gemma2:9b model (this may take 3-5 minutes)...\")\n",
    "!ollama pull gemma2:9b\n",
    "print(\"\\n‚úÖ Model ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Load Cognitive Actions and Variation Pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add datagen to Python path\n",
    "import sys\n",
    "datagen_dir = os.path.abspath('datagen')\n",
    "if datagen_dir not in sys.path:\n",
    "    sys.path.insert(0, datagen_dir)\n",
    "\n",
    "# Import cognitive actions and variation pools\n",
    "from variable_pools import (\n",
    "    COGNITIVE_ACTIONS,\n",
    "    DOMAINS,\n",
    "    TRIGGERS,\n",
    "    EMOTIONAL_STATES,\n",
    "    LANGUAGE_STYLES\n",
    ")\n",
    "\n",
    "# Load sentence starters\n",
    "with open('datagen/all_truncated_outputs.json', 'r') as f:\n",
    "    SENTENCE_STARTERS = [s for s in json.load(f) if s and len(s) > 2]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(COGNITIVE_ACTIONS)} cognitive actions\")\n",
    "print(f\"‚úÖ Loaded {len(DOMAINS)} domains\")\n",
    "print(f\"‚úÖ Loaded {len(TRIGGERS)} triggers\")\n",
    "print(f\"‚úÖ Loaded {len(EMOTIONAL_STATES)} emotional states\")\n",
    "print(f\"‚úÖ Loaded {len(LANGUAGE_STYLES)} language styles\")\n",
    "print(f\"‚úÖ Loaded {len(SENTENCE_STARTERS)} sentence starters\\n\")\n",
    "\n",
    "print(\"Cognitive actions (all will have equal representation):\")\n",
    "for idx, (action, desc) in enumerate(COGNITIVE_ACTIONS.items(), 1):\n",
    "    print(f\"{idx:2d}. {action:30s} - {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Mount Google Drive (for checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "checkpoint_dir = f'/content/drive/MyDrive/cognitive_stratified_data_{timestamp}'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "print(f\"‚úÖ Checkpoints will be saved to: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Stratified Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass VariedExample:\n    text: str\n    cognitive_action: str\n    domain: str\n    trigger: str\n    emotional_state: str\n    language_style: str\n    sentence_starter: str\n    \nclass StratifiedDataGenerator:\n    def __init__(self, base_url=\"http://localhost:11434\", max_parallel=8):\n        self.base_url = base_url\n        self.max_parallel = max_parallel\n        self.semaphore = asyncio.Semaphore(max_parallel)\n        \n        # Store variation pools\n        self.cognitive_actions = COGNITIVE_ACTIONS\n        self.domains = DOMAINS\n        self.triggers = TRIGGERS\n        self.emotional_states = EMOTIONAL_STATES\n        self.language_styles = LANGUAGE_STYLES\n        self.sentence_starters = SENTENCE_STARTERS\n    \n    def calculate_stratified_distribution(self, total_examples: int) -> Dict[str, int]:\n        \"\"\"Calculate how many examples per cognitive action for stratified sampling.\"\"\"\n        num_actions = len(self.cognitive_actions)\n        base_per_action = total_examples // num_actions\n        remainder = total_examples % num_actions\n        \n        distribution = {}\n        for idx, action in enumerate(self.cognitive_actions.keys()):\n            # Distribute remainder across first few actions\n            distribution[action] = base_per_action + (1 if idx < remainder else 0)\n        \n        return distribution\n    \n    def create_prompt(self, action: str, action_desc: str, domain: str,\n                     trigger: str, emotional_state: str, language_style: str,\n                     sentence_starter: str) -> str:\n        \"\"\"Create varied first-person prompt with rich context.\"\"\"\n        # Randomly decide whether to use sentence starter (50% of the time)\n        use_starter = random.random() < 0.5\n        \n        starter_instruction = \"\"\n        if use_starter:\n            starter_instruction = f\"\\n- Start the example with: '{sentence_starter}'\"\n        \n        return f\"\"\"Generate a simple, first-person example of someone {action}.\n\nAction: {action}\nDescription: {action_desc}\nDomain: {domain}\nTrigger: {trigger}\nEmotional state: {emotional_state}\nLanguage style: {language_style}\n\nRequirements:\n- Write in first person (I, my, me)\n- Keep it simple and realistic\n- 2-5 sentences maximum\n- Focus on the {action} cognitive action\n- Use the {language_style} language style\n- Incorporate the trigger and emotional state naturally{starter_instruction}\n- Make it feel natural and relatable\n- Show the cognitive process, not just state it\n\nExample only (no explanation):\"\"\"\n    \n    async def generate_one(self, session: aiohttp.ClientSession, action: str,\n                          action_desc: str, domain: str, trigger: str,\n                          emotional_state: str, language_style: str,\n                          sentence_starter: str, model: str) -> VariedExample:\n        \"\"\"Generate one varied example.\"\"\"\n        async with self.semaphore:\n            prompt = self.create_prompt(action, action_desc, domain, trigger,\n                                       emotional_state, language_style, sentence_starter)\n\n            try:\n                async with session.post(\n                    f\"{self.base_url}/api/generate\",\n                    json={\"model\": model, \"prompt\": prompt, \"stream\": False},\n                    timeout=aiohttp.ClientTimeout(total=60)\n                ) as response:\n                    result = await response.json()\n                    text = result.get('response', '').strip()\n\n                    # Clean up the text\n                    text = text.replace('\"', '').strip()\n                    if not text or len(text) < 20:\n                        return None\n\n                    return VariedExample(\n                        text=text,\n                        cognitive_action=action,\n                        domain=domain,\n                        trigger=trigger,\n                        emotional_state=emotional_state,\n                        language_style=language_style,\n                        sentence_starter=sentence_starter\n                    )\n            except Exception as e:\n                return None\n    \n    async def generate_batch_async(self, count: int, action: str,\n                                  action_desc: str, model: str, pbar=None) -> List[VariedExample]:\n        \"\"\"Generate a batch of examples with rich variation.\"\"\"\n        async with aiohttp.ClientSession() as session:\n            tasks = []\n            for _ in range(count):\n                # Randomly select variations for each example\n                domain = random.choice(self.domains)\n                trigger = random.choice(self.triggers)\n                emotional_state = random.choice(self.emotional_states)\n                language_style = random.choice(self.language_styles)\n                sentence_starter = random.choice(self.sentence_starters)\n\n                task = self.generate_one(session, action, action_desc, domain,\n                                       trigger, emotional_state, language_style,\n                                       sentence_starter, model)\n                tasks.append(task)\n\n            results = await asyncio.gather(*tasks)\n            valid_results = [r for r in results if r is not None]\n            \n            # Update progress bar after batch completes\n            if pbar:\n                pbar.update(len(valid_results))\n            \n            return valid_results\n    \n    def generate_batch(self, count: int, action: str, action_desc: str,\n                      model: str, pbar=None) -> List[VariedExample]:\n        \"\"\"Synchronous wrapper for batch generation.\"\"\"\n        return asyncio.run(self.generate_batch_async(count, action, action_desc, model, pbar))\n    \n    def generate_stratified(self, total_examples: int, model: str,\n                           checkpoint_dir: str) -> List[VariedExample]:\n        \"\"\"Generate stratified examples across all cognitive actions.\"\"\"\n        # Calculate distribution\n        distribution = self.calculate_stratified_distribution(total_examples)\n        \n        print(\"\\nüìä STRATIFIED DISTRIBUTION\")\n        print(\"=\"*60)\n        print(f\"Total examples to generate: {total_examples:,}\")\n        print(f\"Cognitive actions: {len(self.cognitive_actions)}\")\n        print(f\"Examples per action: {min(distribution.values())} - {max(distribution.values())}\")\n        print(\"=\"*60 + \"\\n\")\n        \n        all_examples = []\n        start_time = time.time()\n        \n        # Progress bar that updates after each batch\n        pbar = tqdm(total=total_examples, desc=\"Generating\", unit=\"examples\")\n        \n        # Generate in smaller batches for better progress tracking\n        batch_size = 25  # Generate 25 examples at a time for smooth progress updates\n        \n        for action_idx, (action, action_desc) in enumerate(self.cognitive_actions.items(), 1):\n            count = distribution[action]\n            \n            # Update progress bar description with current action\n            pbar.set_description(f\"[{action_idx}/45] {action[:20]}\")\n            \n            # Generate in smaller batches\n            num_batches = (count + batch_size - 1) // batch_size\n            \n            for batch_idx in range(num_batches):\n                batch_count = min(batch_size, count - (batch_idx * batch_size))\n                \n                # Generate batch (progress bar updates inside)\n                batch_examples = self.generate_batch(\n                    count=batch_count,\n                    action=action,\n                    action_desc=action_desc,\n                    model=model,\n                    pbar=pbar\n                )\n                \n                all_examples.extend(batch_examples)\n                \n                # Update progress info\n                elapsed = time.time() - start_time\n                rate = len(all_examples) / elapsed if elapsed > 0 else 0\n                remaining = total_examples - len(all_examples)\n                eta = remaining / rate if rate > 0 else 0\n                \n                pbar.set_postfix({\n                    'rate': f'{rate:.1f}/s',\n                    'ETA': f'{eta/60:.0f}m'\n                })\n        \n        pbar.close()\n        \n        # Save checkpoint\n        checkpoint_file = os.path.join(\n            checkpoint_dir,\n            f\"stratified_{total_examples}_{int(time.time())}.jsonl\"\n        )\n        \n        with open(checkpoint_file, 'w') as f:\n            for ex in all_examples:\n                f.write(json.dumps(asdict(ex)) + '\\n')\n        \n        print(f\"\\n‚úÖ Saved to: {checkpoint_file}\")\n        \n        return all_examples\n\nprint(\"‚úÖ Stratified data generator ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Configure Generation\n",
    "\n",
    "**Set the number of examples you want to generate.**\n",
    "\n",
    "The generator will automatically distribute them evenly across all 45 cognitive actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE THIS: Set your desired total examples\n",
    "# ============================================================\n",
    "TOTAL_EXAMPLES = 4500  # Will generate 100 per action (4500 / 45 = 100)\n",
    "# Try: 450 (10 each), 2250 (50 each), 4500 (100 each), 9000 (200 each), etc.\n",
    "# ============================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'total_examples': TOTAL_EXAMPLES,\n",
    "    'model': 'gemma2:9b',\n",
    "    'max_parallel': 8,\n",
    "    'checkpoint_dir': checkpoint_dir\n",
    "}\n",
    "\n",
    "# Calculate distribution\n",
    "num_actions = len(COGNITIVE_ACTIONS)\n",
    "examples_per_action = TOTAL_EXAMPLES // num_actions\n",
    "estimated_minutes = TOTAL_EXAMPLES / CONFIG['max_parallel'] * 20 / 60\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STRATIFIED GENERATION CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total examples: {CONFIG['total_examples']:,}\")\n",
    "print(f\"Cognitive actions: {num_actions}\")\n",
    "print(f\"Examples per action: ~{examples_per_action}\")\n",
    "print(f\"Model: {CONFIG['model']} (~5GB VRAM)\")\n",
    "print(f\"Parallel requests: {CONFIG['max_parallel']}\")\n",
    "print(f\"\\nVariation dimensions:\")\n",
    "print(f\"  - Domains: {len(DOMAINS)}\")\n",
    "print(f\"  - Triggers: {len(TRIGGERS)}\")\n",
    "print(f\"  - Emotional states: {len(EMOTIONAL_STATES)}\")\n",
    "print(f\"  - Language styles: {len(LANGUAGE_STYLES)}\")\n",
    "print(f\"  - Sentence starters: {len(SENTENCE_STARTERS)}\")\n",
    "print(f\"\\nPerspective: First-person only\")\n",
    "print(f\"Complexity: Simple only\")\n",
    "print(f\"\\nEstimated time: {estimated_minutes:.0f} minutes ({estimated_minutes/60:.1f} hours)\")\n",
    "print(f\"Expected VRAM: 12-14GB\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Generate Stratified Data\n",
    "\n",
    "Run this cell to generate your stratified dataset. You can run it multiple times with different totals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator\n",
    "generator = StratifiedDataGenerator(max_parallel=CONFIG['max_parallel'])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"üöÄ GENERATING {CONFIG['total_examples']:,} STRATIFIED EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate\n",
    "examples = generator.generate_stratified(\n",
    "    total_examples=CONFIG['total_examples'],\n",
    "    model=CONFIG['model'],\n",
    "    checkpoint_dir=CONFIG['checkpoint_dir']\n",
    ")\n",
    "\n",
    "# Calculate statistics\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ GENERATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total examples generated: {len(examples):,}\")\n",
    "print(f\"Time elapsed: {elapsed/60:.1f} minutes ({elapsed/3600:.2f} hours)\")\n",
    "print(f\"Average rate: {len(examples)/elapsed:.1f} examples/sec\")\n",
    "print(f\"\\nData saved to: {CONFIG['checkpoint_dir']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify stratification\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([asdict(ex) for ex in examples])\n",
    "distribution = df['cognitive_action'].value_counts().sort_index()\n",
    "\n",
    "print(\"\\nüìä DISTRIBUTION VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Min examples per action: {distribution.min()}\")\n",
    "print(f\"Max examples per action: {distribution.max()}\")\n",
    "print(f\"Mean examples per action: {distribution.mean():.1f}\")\n",
    "print(f\"\\nTop 10 cognitive actions:\")\n",
    "print(distribution.head(10))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Preview Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame([asdict(ex) for ex in examples])\n",
    "\n",
    "print(\"üìä DATASET PREVIEW\\n\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "\n",
    "print(f\"\\nüìà Cognitive actions distribution:\")\n",
    "print(df['cognitive_action'].value_counts().sort_values(ascending=False))\n",
    "\n",
    "print(f\"\\nüåç Domain distribution (top 10):\")\n",
    "print(df['domain'].value_counts().head(10))\n",
    "\n",
    "print(f\"\\nüé® Language style distribution:\")\n",
    "print(df['language_style'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Random examples:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for _, row in df.sample(min(5, len(df))).iterrows():\n",
    "    print(f\"\\n[{row['cognitive_action']}]\")\n",
    "    print(f\"Domain: {row['domain']}\")\n",
    "    print(f\"Trigger: {row['trigger'][:50]}...\" if len(row['trigger']) > 50 else f\"Trigger: {row['trigger']}\")\n",
    "    print(f\"Style: {row['language_style']}\")\n",
    "    print(f\"Text: {row['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import glob\n",
    "\n",
    "# Find the most recent stratified file\n",
    "stratified_files = glob.glob(os.path.join(CONFIG['checkpoint_dir'], 'stratified_*.jsonl'))\n",
    "\n",
    "if stratified_files:\n",
    "    latest_file = max(stratified_files, key=os.path.getctime)\n",
    "    print(f\"Downloading: {os.path.basename(latest_file)}\")\n",
    "    print(f\"Size: {os.path.getsize(latest_file) / (1024*1024):.1f} MB\")\n",
    "    files.download(latest_file)\n",
    "    print(\"‚úÖ Download started!\")\n",
    "else:\n",
    "    print(\"‚ùå No stratified file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Generate Multiple Batches (Optional)\n",
    "\n",
    "Run this if you want to generate multiple batches with different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple batch sizes\n",
    "BATCH_SIZES = [450, 2250, 4500]  # 10, 50, 100 per action\n",
    "\n",
    "generator = StratifiedDataGenerator(max_parallel=CONFIG['max_parallel'])\n",
    "\n",
    "for batch_size in BATCH_SIZES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Generating batch of {batch_size:,} examples...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    examples = generator.generate_stratified(\n",
    "        total_examples=batch_size,\n",
    "        model=CONFIG['model'],\n",
    "        checkpoint_dir=CONFIG['checkpoint_dir']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Completed: {len(examples):,} examples\")\n",
    "\n",
    "print(\"\\nüéâ All batches complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Complete!\n",
    "\n",
    "You now have a **stratified dataset** with equal representation across all 45 cognitive actions!\n",
    "\n",
    "### Key benefits:\n",
    "- ‚úÖ **Consistent distribution**: Every batch has the same proportions\n",
    "- ‚úÖ **Flexible batch sizes**: Generate 100, 1000, 10000, or any number\n",
    "- ‚úÖ **Reproducible**: Same distribution every time\n",
    "- ‚úÖ **Rich variation**: 5 dimensions of variation per example\n",
    "\n",
    "### Example distributions:\n",
    "- **450 examples** = 10 per action\n",
    "- **2,250 examples** = 50 per action\n",
    "- **4,500 examples** = 100 per action\n",
    "- **9,000 examples** = 200 per action\n",
    "- **45,000 examples** = 1,000 per action\n",
    "\n",
    "### Files created:\n",
    "- `stratified_{count}_{timestamp}.jsonl` for each generation\n",
    "\n",
    "### Example format:\n",
    "```json\n",
    "{\n",
    "  \"text\": \"I need to analyze my spending patterns...\",\n",
    "  \"cognitive_action\": \"analyzing\",\n",
    "  \"domain\": \"financial planning\",\n",
    "  \"trigger\": \"reviewing monthly bank statement\",\n",
    "  \"emotional_state\": \"feeling anxious about the implications\",\n",
    "  \"language_style\": \"straightforward and direct\",\n",
    "  \"sentence_starter\": \"I can see that\"\n",
    "}\n",
    "```\n",
    "\n",
    "### To generate more:\n",
    "1. Change `TOTAL_EXAMPLES` in cell 7\n",
    "2. Re-run cells 7 and 8\n",
    "3. Download the new file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}