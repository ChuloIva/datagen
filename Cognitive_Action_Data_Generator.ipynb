{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Cognitive Action Training Data Generator\n",
    "\n",
    "This notebook generates high-quality training data for cognitive action recognition using Ollama locally or via API.\n",
    "\n",
    "**Based on Scientific Taxonomies:**\n",
    "- Bloom's Taxonomy (cognitive processes)\n",
    "- Guilford's Structure of Intellect\n",
    "- Krathwohl's Affective Domain\n",
    "- Gross's Emotion Regulation Model\n",
    "- Metacognitive Process Frameworks\n",
    "\n",
    "**Target:** 100,000+ diverse examples of explicit cognitive and psychological actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install requests pandas numpy tqdm matplotlib seaborn\n",
    "\n",
    "# If running locally with Ollama installed:\n",
    "# !pip install ollama\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_files"
   },
   "source": "## 2. Import Supporting Modules\n\nThe following Python modules are part of this repository:\n- `variable_pools.py` - Contains cognitive action taxonomies and variable pools\n- `prompt_templates.py` - Template system for generating diverse prompts\n- `data_generator.py` - Main data generation engine\n\nThese files are already in the `datagen/` directory and will be imported in the cells below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_variable_pools"
   },
   "outputs": [],
   "source": "# Import variable pools module\nimport sys\nimport os\n\n# Ensure the datagen directory is in the Python path\ncurrent_dir = os.path.dirname(os.path.abspath('__file__'))\nif current_dir not in sys.path:\n    sys.path.insert(0, current_dir)\n\n# Import from the repository\nfrom variable_pools import *\n\nprint(\"Variable pools loaded successfully!\")\nprint(f\"Total cognitive actions: {len(COGNITIVE_ACTIONS)}\")\nprint(f\"Total domains: {len(DOMAINS)}\")\nprint(f\"Total subjects: {len(SUBJECTS)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_prompt_templates"
   },
   "outputs": [],
   "source": "# Import prompt templates module\nfrom prompt_templates import *\n\nprint(\"Prompt templates loaded successfully!\")\nprint(f\"Single action templates: {len(SINGLE_ACTION_TEMPLATES)}\")\nprint(f\"Chain templates: {len(CHAIN_TEMPLATES)}\")\nprint(f\"Dialogue templates: {len(DIALOGUE_TEMPLATES)}\")\nprint(f\"Thought stream templates: {len(THOUGHT_STREAM_TEMPLATES)}\")\nprint(f\"Negative templates: {len(NEGATIVE_TEMPLATES)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ollama_setup"
   },
   "source": [
    "## 3. Ollama Setup\n",
    "\n",
    "Choose one of the following options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ollama_local"
   },
   "source": [
    "### Option A: Local Ollama Installation (Recommended)\n",
    "\n",
    "If you have Ollama running locally, modify the URL below to point to your instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_ollama_local"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "class OllamaClient:\n",
    "    def __init__(self, base_url=\"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "    \n",
    "    def generate(self, model=\"llama3.2\", prompt=\"\", stream=False):\n",
    "        \"\"\"Generate text using Ollama API\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(url, json=data, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            if stream:\n",
    "                return response.iter_lines()\n",
    "            else:\n",
    "                return response.json()\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error connecting to Ollama: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List available models\"\"\"\n",
    "        url = f\"{self.base_url}/api/tags\"\n",
    "        try:\n",
    "            response = self.session.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error connecting to Ollama: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize Ollama client\n",
    "# CHANGE THIS URL TO YOUR OLLAMA INSTANCE\n",
    "ollama = OllamaClient(base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Test connection\n",
    "models = ollama.list_models()\n",
    "if models:\n",
    "    print(\"✅ Connected to Ollama successfully!\")\n",
    "    print(\"Available models:\")\n",
    "    for model in models.get('models', []):\n",
    "        print(f\"  - {model.get('name', 'Unknown')}\")\n",
    "else:\n",
    "    print(\"❌ Could not connect to Ollama. Please check your setup.\")\n",
    "    print(\"Make sure Ollama is running and accessible at the specified URL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ollama_colab"
   },
   "source": [
    "### Option B: Install Ollama in Colab (Experimental)\n",
    "\n",
    "⚠️ **Note:** This is experimental and may not work reliably in all Colab environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_ollama_colab"
   },
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to install Ollama directly in Colab\n",
    "# This is experimental and may not work in all environments\n",
    "\n",
    "# !curl -fsSL https://ollama.ai/install.sh | sh\n",
    "# \n",
    "# # Start Ollama in background\n",
    "# import subprocess\n",
    "# import time\n",
    "# \n",
    "# # Start Ollama server\n",
    "# ollama_process = subprocess.Popen(['ollama', 'serve'], \n",
    "#                                   stdout=subprocess.PIPE, \n",
    "#                                   stderr=subprocess.PIPE)\n",
    "# \n",
    "# # Wait for server to start\n",
    "# time.sleep(10)\n",
    "# \n",
    "# # Pull a model\n",
    "# !ollama pull llama3.2\n",
    "# \n",
    "# print(\"Ollama installed and model pulled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_generation"
   },
   "source": [
    "## 4. Data Generation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_data_generator"
   },
   "outputs": [],
   "source": "# Import data generator module\nfrom data_generator import *\n\nprint(\"Data generator loaded successfully!\")\nprint(\"CognitiveDataGenerator class ready to use\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_modules"
   },
   "outputs": [],
   "source": "# Verify all modules are imported and working\nprint(\"All modules imported successfully!\")\nprint(f\"Ready to generate data with {len(COGNITIVE_ACTIONS)} cognitive actions\")\nprint(f\"\\nAvailable components:\")\nprint(f\"  - Variable pools: ✓\")\nprint(f\"  - Prompt templates: ✓\")\nprint(f\"  - Data generator: ✓\")\nprint(f\"  - Ollama client: {'✓' if 'ollama' in dir() else '⚠️ Configure in section 3'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_generation"
   },
   "source": [
    "## 5. Test the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_prompt_generation"
   },
   "outputs": [],
   "source": [
    "# Test prompt generation without LLM\n",
    "print(\"Testing prompt generation...\\n\")\n",
    "\n",
    "# Generate a few sample prompts\n",
    "for i in range(3):\n",
    "    prompt, params = generate_prompt(template_type=\"single\", iteration_number=i)\n",
    "    print(f\"=== PROMPT {i+1} ===\")\n",
    "    print(f\"Cognitive Action: {params['cognitive_action']}\")\n",
    "    print(f\"Domain: {params['domain']}\")\n",
    "    print(f\"Subject: {params['subject']}\")\n",
    "    print()\n",
    "    print(prompt)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_with_ollama"
   },
   "outputs": [],
   "source": [
    "# Test with Ollama (if connected)\n",
    "generator = CognitiveDataGenerator(ollama_client=ollama)\n",
    "\n",
    "print(\"Testing single example generation...\")\n",
    "\n",
    "# Generate a single example\n",
    "example = generator.generate_single_example(\n",
    "    cognitive_action=\"reconsidering\",\n",
    "    template_type=\"single\",\n",
    "    model=\"llama3.2\"  # Change this to your available model\n",
    ")\n",
    "\n",
    "if example:\n",
    "    print(\"\\n=== GENERATED EXAMPLE ===\")\n",
    "    print(f\"Cognitive Action: {example.primary_cognitive_action}\")\n",
    "    print(f\"Domain: {example.domain}\")\n",
    "    print(f\"Complexity: {example.complexity}\")\n",
    "    print(f\"Format: {example.format_type}\")\n",
    "    print()\n",
    "    print(\"Generated Text:\")\n",
    "    print(example.text)\n",
    "    print()\n",
    "    print(\"Metadata:\")\n",
    "    for key, value in example.metadata.items():\n",
    "        if key != 'prompt_used':  # Skip the long prompt\n",
    "            print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"Failed to generate example. Check Ollama connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_generation"
   },
   "source": [
    "## 6. Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "small_batch_test"
   },
   "outputs": [],
   "source": [
    "# Generate a small batch for testing\n",
    "print(\"Generating small test batch...\")\n",
    "\n",
    "test_examples = generator.generate_batch(\n",
    "    batch_size=5,\n",
    "    cognitive_action=None,  # Random actions\n",
    "    template_type=\"single\",\n",
    "    model=\"llama3.2\",\n",
    "    delay=1.0  # 1 second delay between generations\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(test_examples)} examples\")\n",
    "\n",
    "# Show a few examples\n",
    "for i, example in enumerate(test_examples[:3]):\n",
    "    print(f\"\\n=== EXAMPLE {i+1} ===\")\n",
    "    print(f\"Action: {example.primary_cognitive_action}\")\n",
    "    print(f\"Domain: {example.domain}\")\n",
    "    print(f\"Text: {example.text[:200]}...\")\n",
    "\n",
    "# Show statistics\n",
    "generator.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stratified_generation"
   },
   "outputs": [],
   "source": [
    "# Generate stratified dataset (smaller for testing)\n",
    "print(\"Generating stratified test dataset...\")\n",
    "\n",
    "# Create new generator for clean stats\n",
    "stratified_generator = CognitiveDataGenerator(ollama_client=ollama)\n",
    "\n",
    "stratified_examples = stratified_generator.generate_stratified_dataset(\n",
    "    total_examples=100,  # Start small for testing\n",
    "    model=\"llama3.2\"\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(stratified_examples)} stratified examples\")\n",
    "stratified_generator.print_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_analysis"
   },
   "source": [
    "## 7. Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze_data"
   },
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "data = []\n",
    "for example in stratified_generator.generated_examples:\n",
    "    data.append({\n",
    "        'text': example.text,\n",
    "        'cognitive_action': example.primary_cognitive_action,\n",
    "        'domain': example.domain,\n",
    "        'complexity': example.complexity,\n",
    "        'format_type': example.format_type,\n",
    "        'text_length': len(example.text),\n",
    "        'word_count': len(example.text.split()),\n",
    "        'subject': example.metadata.get('subject', ''),\n",
    "        'emotional_state': example.metadata.get('emotional_state', '')\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_data"
   },
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Distribution of cognitive actions\n",
    "plt.subplot(2, 3, 1)\n",
    "cognitive_action_counts = df['cognitive_action'].value_counts()\n",
    "plt.bar(range(len(cognitive_action_counts)), cognitive_action_counts.values)\n",
    "plt.xticks(range(len(cognitive_action_counts)), cognitive_action_counts.index, rotation=45, ha='right')\n",
    "plt.title('Distribution of Cognitive Actions')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Distribution of domains\n",
    "plt.subplot(2, 3, 2)\n",
    "domain_counts = df['domain'].value_counts()[:10]  # Top 10\n",
    "plt.bar(range(len(domain_counts)), domain_counts.values)\n",
    "plt.xticks(range(len(domain_counts)), domain_counts.index, rotation=45, ha='right')\n",
    "plt.title('Top 10 Domains')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Distribution of complexity levels\n",
    "plt.subplot(2, 3, 3)\n",
    "complexity_counts = df['complexity'].value_counts()\n",
    "plt.pie(complexity_counts.values, labels=complexity_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Complexity Distribution')\n",
    "\n",
    "# Text length distribution\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.hist(df['text_length'], bins=20, edgecolor='black')\n",
    "plt.title('Text Length Distribution')\n",
    "plt.xlabel('Characters')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Word count distribution\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.hist(df['word_count'], bins=20, edgecolor='black')\n",
    "plt.title('Word Count Distribution')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Format type distribution\n",
    "plt.subplot(2, 3, 6)\n",
    "format_counts = df['format_type'].value_counts()\n",
    "plt.pie(format_counts.values, labels=format_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Format Type Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show some sample texts by cognitive action\n",
    "print(\"\\n=== SAMPLE TEXTS BY COGNITIVE ACTION ===\")\n",
    "for action in df['cognitive_action'].unique()[:5]:  # First 5 actions\n",
    "    sample = df[df['cognitive_action'] == action]['text'].iloc[0]\n",
    "    print(f\"\\n{action.upper()}:\")\n",
    "    print(f\"  {sample[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export_data"
   },
   "source": [
    "## 8. Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_datasets"
   },
   "outputs": [],
   "source": [
    "# Export the generated dataset\n",
    "timestamp = int(time.time())\n",
    "\n",
    "# Export as JSONL (recommended for large datasets)\n",
    "jsonl_filename = f\"cognitive_actions_dataset_{timestamp}.jsonl\"\n",
    "stratified_generator.export_dataset(jsonl_filename, format=\"jsonl\")\n",
    "\n",
    "# Export as JSON (for smaller datasets)\n",
    "json_filename = f\"cognitive_actions_dataset_{timestamp}.json\"\n",
    "stratified_generator.export_dataset(json_filename, format=\"json\")\n",
    "\n",
    "# Export as CSV for analysis\n",
    "csv_filename = f\"cognitive_actions_analysis_{timestamp}.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"\\nDataset exported:\")\n",
    "print(f\"  JSONL: {jsonl_filename}\")\n",
    "print(f\"  JSON: {json_filename}\")\n",
    "print(f\"  CSV: {csv_filename}\")\n",
    "\n",
    "# Show file sizes\n",
    "import os\n",
    "for filename in [jsonl_filename, json_filename, csv_filename]:\n",
    "    if os.path.exists(filename):\n",
    "        size = os.path.getsize(filename)\n",
    "        print(f\"  {filename}: {size:,} bytes ({size/1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "large_scale_generation"
   },
   "source": [
    "## 9. Large Scale Generation\n",
    "\n",
    "⚠️ **Warning:** Large scale generation can take hours and consume significant computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configure_large_scale"
   },
   "outputs": [],
   "source": [
    "# Configuration for large-scale generation\n",
    "LARGE_SCALE_CONFIG = {\n",
    "    'phase1_round1': 10000,  # Core cognitive actions\n",
    "    'phase1_round2': 5000,   # Action combinations  \n",
    "    'phase2': 10000,         # Domain variations\n",
    "    'phase3': 10000,         # Complexity variations\n",
    "    'phase4': 2000,          # Negative examples\n",
    "    'phase5': 2000,          # Dialogue format\n",
    "    'phase6': 1000,          # Thought-stream format\n",
    "    'model': 'llama3.2',\n",
    "    'delay': 0.1,            # Delay between generations (seconds)\n",
    "    'checkpoint_interval': 100  # Save progress every N examples\n",
    "}\n",
    "\n",
    "print(\"Large scale configuration:\")\n",
    "total_target = sum(v for k, v in LARGE_SCALE_CONFIG.items() if k.startswith('phase'))\n",
    "print(f\"Total target examples: {total_target:,}\")\n",
    "print(f\"Estimated time (with {LARGE_SCALE_CONFIG['delay']}s delay): {total_target * LARGE_SCALE_CONFIG['delay'] / 3600:.1f} hours\")\n",
    "\n",
    "for phase, count in LARGE_SCALE_CONFIG.items():\n",
    "    if phase.startswith('phase'):\n",
    "        print(f\"  {phase}: {count:,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_large_scale"
   },
   "outputs": [],
   "source": [
    "# Uncomment and run this cell for large-scale generation\n",
    "# WARNING: This will take a very long time!\n",
    "\n",
    "# def run_large_scale_generation():\n",
    "#     \"\"\"Run the complete large-scale generation pipeline\"\"\"\n",
    "#     \n",
    "#     large_generator = CognitiveDataGenerator(ollama_client=ollama)\n",
    "#     \n",
    "#     phases = [\n",
    "#         ('phase1_round1', 'single', LARGE_SCALE_CONFIG['phase1_round1']),\n",
    "#         ('phase1_round2', 'chain', LARGE_SCALE_CONFIG['phase1_round2']),\n",
    "#         ('phase2', 'single', LARGE_SCALE_CONFIG['phase2']),\n",
    "#         ('phase3', 'single', LARGE_SCALE_CONFIG['phase3']),\n",
    "#         ('phase4', 'negative', LARGE_SCALE_CONFIG['phase4']),\n",
    "#         ('phase5', 'dialogue', LARGE_SCALE_CONFIG['phase5']),\n",
    "#         ('phase6', 'thought_stream', LARGE_SCALE_CONFIG['phase6'])\n",
    "#     ]\n",
    "#     \n",
    "#     for phase_name, template_type, target_count in phases:\n",
    "#         print(f\"\\n🚀 Starting {phase_name}: {target_count:,} examples\")\n",
    "#         print(f\"Template type: {template_type}\")\n",
    "#         \n",
    "#         phase_examples = large_generator.generate_batch(\n",
    "#             batch_size=target_count,\n",
    "#             template_type=template_type,\n",
    "#             model=LARGE_SCALE_CONFIG['model'],\n",
    "#             delay=LARGE_SCALE_CONFIG['delay']\n",
    "#         )\n",
    "#         \n",
    "#         print(f\"✅ Completed {phase_name}: {len(phase_examples):,} examples\")\n",
    "#         \n",
    "#         # Checkpoint save\n",
    "#         checkpoint_filename = f\"checkpoint_{phase_name}_{int(time.time())}.jsonl\"\n",
    "#         large_generator.export_dataset(checkpoint_filename, format=\"jsonl\")\n",
    "#         print(f\"💾 Checkpoint saved: {checkpoint_filename}\")\n",
    "#         \n",
    "#         # Print progress statistics\n",
    "#         large_generator.print_statistics()\n",
    "#     \n",
    "#     # Final export\n",
    "#     final_timestamp = int(time.time())\n",
    "#     final_filename = f\"cognitive_actions_complete_dataset_{final_timestamp}.jsonl\"\n",
    "#     large_generator.export_dataset(final_filename, format=\"jsonl\")\n",
    "#     \n",
    "#     print(f\"\\n🎉 COMPLETE! Generated {len(large_generator.generated_examples):,} examples\")\n",
    "#     print(f\"📁 Final dataset: {final_filename}\")\n",
    "#     \n",
    "#     return large_generator\n",
    "\n",
    "# # Uncomment the line below to start large-scale generation\n",
    "# # large_generator = run_large_scale_generation()\n",
    "\n",
    "print(\"Large-scale generation function defined.\")\n",
    "print(\"Uncomment the last line and run this cell to start large-scale generation.\")\n",
    "print(\"⚠️  Make sure you have sufficient time and resources before starting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quality_control"
   },
   "source": [
    "## 10. Quality Control and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quality_checks"
   },
   "outputs": [],
   "source": [
    "def quality_control_analysis(examples):\n",
    "    \"\"\"Perform quality control analysis on generated examples\"\"\"\n",
    "    \n",
    "    print(\"=== QUALITY CONTROL ANALYSIS ===\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_examples = len(examples)\n",
    "    print(f\"Total examples analyzed: {total_examples:,}\")\n",
    "    \n",
    "    # Text length analysis\n",
    "    text_lengths = [len(ex.text) for ex in examples]\n",
    "    word_counts = [len(ex.text.split()) for ex in examples]\n",
    "    \n",
    "    print(f\"\\nText Length Statistics:\")\n",
    "    print(f\"  Average characters: {np.mean(text_lengths):.1f}\")\n",
    "    print(f\"  Average words: {np.mean(word_counts):.1f}\")\n",
    "    print(f\"  Min/Max characters: {min(text_lengths)} / {max(text_lengths)}\")\n",
    "    print(f\"  Min/Max words: {min(word_counts)} / {max(word_counts)}\")\n",
    "    \n",
    "    # Coverage analysis\n",
    "    cognitive_actions = set(ex.primary_cognitive_action for ex in examples)\n",
    "    domains = set(ex.domain for ex in examples)\n",
    "    \n",
    "    print(f\"\\nCoverage Statistics:\")\n",
    "    print(f\"  Cognitive actions covered: {len(cognitive_actions)}/{len(COGNITIVE_ACTIONS)} ({len(cognitive_actions)/len(COGNITIVE_ACTIONS)*100:.1f}%)\")\n",
    "    print(f\"  Domains covered: {len(domains)}/{len(DOMAINS)} ({len(domains)/len(DOMAINS)*100:.1f}%)\")\n",
    "    \n",
    "    # Quality indicators\n",
    "    print(f\"\\nQuality Indicators:\")\n",
    "    \n",
    "    # Check for very short examples\n",
    "    very_short = sum(1 for length in text_lengths if length < 50)\n",
    "    print(f\"  Very short examples (<50 chars): {very_short} ({very_short/total_examples*100:.1f}%)\")\n",
    "    \n",
    "    # Check for very long examples\n",
    "    very_long = sum(1 for length in text_lengths if length > 1000)\n",
    "    print(f\"  Very long examples (>1000 chars): {very_long} ({very_long/total_examples*100:.1f}%)\")\n",
    "    \n",
    "    # Check for examples that might contain prompt artifacts\n",
    "    prompt_artifacts = sum(1 for ex in examples if any(word in ex.text.lower() for word in \n",
    "                          ['generate', 'example', 'sentence', 'requirement', 'output']))\n",
    "    print(f\"  Potential prompt artifacts: {prompt_artifacts} ({prompt_artifacts/total_examples*100:.1f}%)\")\n",
    "    \n",
    "    # Diversity check - look for repeated phrases\n",
    "    all_texts = [ex.text for ex in examples]\n",
    "    unique_texts = set(all_texts)\n",
    "    print(f\"  Unique texts: {len(unique_texts)}/{total_examples} ({len(unique_texts)/total_examples*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'total_examples': total_examples,\n",
    "        'avg_length': np.mean(text_lengths),\n",
    "        'avg_words': np.mean(word_counts),\n",
    "        'cognitive_actions_covered': len(cognitive_actions),\n",
    "        'domains_covered': len(domains),\n",
    "        'very_short_pct': very_short/total_examples*100,\n",
    "        'very_long_pct': very_long/total_examples*100,\n",
    "        'prompt_artifacts_pct': prompt_artifacts/total_examples*100,\n",
    "        'uniqueness_pct': len(unique_texts)/total_examples*100\n",
    "    }\n",
    "\n",
    "# Run quality control on current examples\n",
    "if stratified_generator.generated_examples:\n",
    "    qc_results = quality_control_analysis(stratified_generator.generated_examples)\n",
    "else:\n",
    "    print(\"No examples to analyze. Generate some examples first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sample_review"
   },
   "outputs": [],
   "source": [
    "# Manual review of sample examples\n",
    "def review_samples(examples, n_samples=5):\n",
    "    \"\"\"Review a random sample of examples for quality\"\"\"\n",
    "    \n",
    "    if not examples:\n",
    "        print(\"No examples to review.\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== MANUAL QUALITY REVIEW ===\")\n",
    "    print(f\"Reviewing {min(n_samples, len(examples))} random examples:\\n\")\n",
    "    \n",
    "    sample_examples = random.sample(examples, min(n_samples, len(examples)))\n",
    "    \n",
    "    for i, example in enumerate(sample_examples, 1):\n",
    "        print(f\"--- EXAMPLE {i} ---\")\n",
    "        print(f\"Cognitive Action: {example.primary_cognitive_action}\")\n",
    "        print(f\"Domain: {example.domain}\")\n",
    "        print(f\"Complexity: {example.complexity}\")\n",
    "        print(f\"Format: {example.format_type}\")\n",
    "        print(f\"Length: {len(example.text)} chars, {len(example.text.split())} words\")\n",
    "        print()\n",
    "        print(\"Text:\")\n",
    "        print(f\"\\\"{example.text}\\\"\")\n",
    "        print()\n",
    "        print(\"Subject:\", example.metadata.get('subject', 'N/A'))\n",
    "        print(\"Emotional State:\", example.metadata.get('emotional_state', 'N/A'))\n",
    "        print(\"Unique Angle:\", example.metadata.get('unique_angle', 'N/A'))\n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Review samples\n",
    "if stratified_generator.generated_examples:\n",
    "    review_samples(stratified_generator.generated_examples, n_samples=3)\n",
    "else:\n",
    "    print(\"No examples to review. Generate some examples first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 🎉 Conclusion\n",
    "\n",
    "You now have a complete cognitive action data generation system!\n",
    "\n",
    "### What You've Built:\n",
    "- **Scientific Foundation**: Based on established taxonomies from cognitive psychology\n",
    "- **Flexible Architecture**: Modular system with variable pools and templates\n",
    "- **Scalable Generation**: Can generate from small batches to 100,000+ examples\n",
    "- **Quality Control**: Built-in analysis and validation tools\n",
    "- **Multiple Formats**: Support for various example types (single actions, chains, dialogues, thought streams)\n",
    "\n",
    "### Next Steps:\n",
    "1. **Scale Up**: Use the large-scale generation for your full dataset\n",
    "2. **Fine-tune**: Adjust templates and variables based on your specific needs\n",
    "3. **Validate**: Run quality control on larger datasets\n",
    "4. **Train Models**: Use the generated data to train your cognitive action recognition models\n",
    "\n",
    "### Tips for Production Use:\n",
    "- Monitor generation quality regularly\n",
    "- Save checkpoints during long generation runs\n",
    "- Experiment with different Ollama models for variety\n",
    "- Consider post-processing for consistency\n",
    "\n",
    "**Happy data generating! 🚀**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}